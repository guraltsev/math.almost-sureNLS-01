\documentclass[10pt,leqno]{amsart}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsmath,amsfonts,amssymb,amsthm} 
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage[all]{xy}
\usepackage{calc}
\usepackage{multicol}
\usepackage{todonotes}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}

\newtheorem{defi}[thm]{Definition}
\newtheorem{lemt}{Lemme technique}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}{Conjecture}[section]
\newtheorem{etape}{Step}[section]
\newtheorem{rmq}[thm]{Remark}
\newtheorem{step}{Step}
\usepackage{appendix}
\usepackage{color}
\usepackage{enumitem}
\usepackage{hyperref}

\makeatletter
\newcommand{\pushright}[1]{\ifmeasuring@#1\else\omit\hfill$\displaystyle#1$\fi\ignorespaces}
\newcommand{\pushleft}[1]{\ifmeasuring@#1\else\omit$\displaystyle#1$\hfill\fi\ignorespaces}
\makeatother

\newcommand{\alert}[1]{\textcolor{red}{#1}}
\renewcommand\minalignsep{5pt}
\newcommand{\R}{\mathbb{R}}
\numberwithin{equation}{section}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Hy}{\mathbb{H}}
\newcommand{\Cc}{\mathbb{C}}
\newcommand{\M}[1]{\mathcal{M}_{#1}(\mathbb{R})}
\newcommand{\as}{\backslash}
\newcommand{\en}[1]{\{1,\dots,#1\}}
\newcommand{\pt}[2]{\frac{\partial #1}{\partial#2}}
\newcommand{\ptd}[2]{\frac{\partial^2 #1}{\partial {#2}^2}}
\newcommand{\ptdd}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\coo}[2]{(#1_1,\dots,#1_{#2})}
\newcommand{\ch}{\text{ch}}
\newcommand{\sh}{\text{sh}}
\newcommand{\lab}{\label}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\fref}{\eqref}
\newcommand{\ra}[4]{ \left\{ \begin{array}{ccc} #1 & \longrightarrow & #2\\ #3 & \longmapsto & #4 \end{array} \right.}
\newcommand{\cad}{c'est-ï¿½-dire }
\newcommand{\ud}{\text{d}}
\newcommand{\e}{\varepsilon}
\newcommand{\atan}{\text{Arctan}}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\theenumii}{\roman{enumii}}
\newcounter{exercice}
\newenvironment{exo}{\par\smallskip \addtocounter{exercice}{1} \textbf{\large Exercice \arabic{exercice}} \\}{\medskip \par}


\begin{document}
\title[Local well-posedness for cubic NLS]{Almost sure local well-posedness for cubic nonlinear Schr\" odinger equation with higher order operators}
\author[Jean-Baptiste Casteras]{Jean-Baptiste Casteras}
\address{CMAFcIO, Faculdade de Ci\^encias da Universidade de Lisboa, Edificio C6, Piso 1, Campo Grande 1749-016 Lisboa, Portugal}
\email{jeanbaptiste.casteras@gmail.com}
\author[Juraj F\" oldes]{Juraj F\" oldes}
\address{Dept. of Mathematics, University of Virginia, 322 Kerchof Hall, Charlottesville, VA 22904-4137}
\email{foldes@virginia.edu}
\author[Gennady Uraltsev]{Gennady Uraltsev}
\address{Dept. of Mathematics, University of Virginia, Kerchof Hall, Charlottesville, VA 22904-4137}
\email{gennady.uraltsev@gmail.com}
\thanks{J.-B.C. supported by FCT - Funda\c c\~ao para a Ci\^encia e a Tecnologia, under the project: UIDB/04561/2020, J. F. was partly supported by grant NSF-DMS-1816408}
\subjclass[2000]{35Q41, 37L50}
\keywords{Schr\"odinger equation; almost-sure local well-posedness; random initial data}


\begin{abstract}
In this paper, we study the local well-posedness of the cubic  Schr\" odinger equation:
$$ (i\partial_t - P) u = \pm |u|^2 u\quad \textrm{ on }\ I\times \R^d ,$$
with randomized initial data, and $P$ being an operator of degree $s \geq 2$. Using careful estimates in anisotropic spaces, we improve and extend known results for the standard Schr\" odinger equation 
(that is, $P$ being  Laplacian) to any dimension under natural assumptions on $P$, whose Fourier symbol might be sign changing. 
Quite interestingly, we  also exhibit the existence of a new regime depending on $s$ and $d$, which was not present for the Laplacian. 

\end{abstract}

\maketitle



\section{Introduction}
We consider a cubic nonlinear Schr\" odinger equation with a general self-adjoint operator $P$ of order $s$ having constant coefficients. More precisely, we investigate 
the local well-posedness of the following problem
\begin{equation}
\label{intro}
\begin{cases} (i\partial_t - P) u = \pm |u|^2 u \qquad \textrm{on} \quad \ I\times \R^d ,
\\ u(0)=f \in H_x^S (\R^d) . \end{cases}
\end{equation}
When $P=-\Delta$, \eqref{intro} is the classical cubic Schr\" odinger equation, whereas if $P= \Delta^2 -\mu \Delta$, $\mu \in \{-1,0,1\}$ then,  
\eqref{intro}  becomes the fourth order Schr\" odinger equation with mixed dispersion introduced by Karpman and Shagalov \cite{MR1779828} (see also \cite{MR1372681}). Notice that if $P= \Delta^{s/2}$,
\eqref{intro} possesses a natural scaling. Specifically, 
if $u$ satisfies the equation in \eqref{intro}, then 
$$u_{\lambda}(t,x)= \lambda^{\frac{s}{2}} u(\lambda^s t, \lambda x),\ \lambda>0 $$
also satisfies the same equation. 
In addition, 
$$\|u_\lambda (0)\|_{\dot{H}^\gamma}= \lambda^{\gamma + \frac{s}{2} - \frac{d}{2}} \|u(0)\|_{\dot{H}^\gamma} = \lambda^{\gamma +\frac{s - d}{2}} \|f\|_{\dot{H}^\gamma}.$$
Denote $S_c = \frac{d-s}{2}$, and based on the above scaling,  for initial data $f \in H^{S} (\R^d)$, we say that the Cauchy problem \eqref{intro} is 
$$
\begin{cases}\text{subcritical}\\  \text{critical}\\ \text{supercritical} \end{cases} \qquad  \textrm{if} \quad S\begin{cases}>S_c,\\ =S_c, \\ <S_c . \end{cases}
$$ 
Our goal is to investigate the local well-posedness of \eqref{intro} for randomized initial data in the supercritical regime. In the subcritical or critical regime, local solutions can be constructed using Strichartz estimates and a classical fixed point argument. We refer to \cite{MR2002047,MR1055532,MR2415387,MR2288737} for some results concerning the classical nonlinear Schr\" odinger equation and to \cite{MR2353631,MR2746203} for results on the fourth order one. On the other hand, in the supercritical regime, problem \eqref{intro} is ill-posed by a result of 
Chirst, Colliander, and Tao \cite{christ2003ill}. However, it is known that suitable randomizations of the initial data can be used to construct local or even global solutions in the supercritical regime. Bourgain \cite{MR1309539,MR1374420}  introduced randomization  to prove existence of solutions to the periodic nonlinear Schr\" odinger equation. His approach was based on the construction of invariant measures (see also \cite{MR939505,MR4312285} for other results in this direction). We also refer to \cite{MR4236191,MR2425134} for more references on related problems.

 In the following, we use a randomization of initial data based on a unit-scale decomposition of frequency space. Let us mention that other randomizations have been investigated in \cite{MR3237443,MR3022846,MR2569900,spitz2021almost}. Let $\psi \in C_c^\infty (\R^d)$ be an even, non-negative cut-off function supported in the unit-ball of $\R^d$ centered at $0$ and such that, for all $\xi \in \R^d$,
$$\sum_{k\in \Z^d} \psi (\xi -k)=1.$$
For $f\in H_x^{\gamma} (\R^d)$, $\gamma\in \R$, and $k\in \Z^d$, define the function $Q_k f : \R^d \rightarrow \mathbb{C}$ as 
\begin{equation}\label{proj1}
(Q_k f)(x)= \mathcal{F}^{-1}\big(\psi (\xi -k) \mathcal{F} (f) (\xi)\big)(x), \qquad   \textrm{for} \quad  x\in \R^d \,,
\end{equation}
where $\mathcal{F}(f)$ stands for the Fourier transform of $f$. Let $(g_k)_{k\in \Z^d}$ be a sequence of independent, standard (zero-mean, unit variance), 
complex valued Gaussian random variables on a probability space 
$(\Omega , \mathcal{A}, \mathbb{P})$. For $f\in H^S_x (\R^d)$, we define its randomization by
\begin{equation}\label{ricc}
f^\omega = \sum_{k\in \Z^d} g_k (\omega) Q_k f.
\end{equation}
Since $Q_k f$ is localized in the Fourier space,  Bernstein inequality (Young's inequality applied to a convolution with a smooth function) implies for 
any $1 \leq r_1 \leq r_2 \leq \infty$ and any $k \in \Z^d$ that 
\begin{equation}\label{usbe}
\|Q_k f\|_{L^{r_2}_x(\R^d)} \leq C(r_1, r_2) \|Q_k f\|_{L^{r_1}_x(\R^d)}
\end{equation} 
with a constant $C$ that is independent of $k$.

Our randomization does not improve the differentiability of $f$ in the sense that if $f\in H^{\gamma}(\R^d) \backslash H^{\gamma+\varepsilon}(\R^d)$ for some $\varepsilon > 0$, then $f^\omega\in H^{\gamma}(\R^d) \backslash H^{\gamma+\varepsilon}(\R^d)$ almost surely, see \cite{MR2425133}. However,  the space-time integrability properties of the linear evolution given by $e^{-itP}f^\omega$ improve compared to 
the deterministic one, see Lemma \ref{lemproba1} below. 

The literature contains several well-posedness results using our randomization. B\' enyi, Oh, and Pocovnicu \cite{MR3350022} proved the almost sure local well-posedness of \eqref{intro} with $P=-\Delta$ for $d\geq 3$ and $S>\frac{d-1}{d+1}\frac{d-2}{2}$ in the following sense: there exist $c,C,\gamma>0$ such that  for each $0<T<<1$, there exists a set $\Omega_T \subset \Omega$ with the following properties :
\begin{itemize}
\item $\mathbb{P}(\Omega_T^c)<C \exp (-\frac{c}{T^\gamma \|f\|_{H^S}^2}).$
 \item For each $\omega \in \Omega_T$, there exists a unique solution u to \eqref{intro} with $u(0)=f^\omega$ in the class
$$e^{-it\Delta}f^\omega +C([-T,T] ; H^{\frac{d-2}{2}}(\R^d)) \subset C([-T,T] ; H^{S}(\R^d)) .$$
\end{itemize}
Later, Brereton \cite{MR3907746} obtained analogous results for $P = -\Delta$ and quintic nonlinearity.  When $d=3$, Shen, Soffer, and Wu \cite{shen2021almost}, very recently, obtained the local well-posedness of \eqref{intro} (with the Laplacian) for $S\geq \frac{1}{6}$ improving \cite{MR3350022}. All the previous results rely on a fixed point argument for operators on variants of the $X^{s,b}$ 
spaces adapted to the variation spaces $V^p$ and \(U^{p}\) introduced by Koch, Tataru, and collaborators \cite{MR2526409, MR2824485, MR3618884}. 
The result of \cite{MR3350022} was also improved by Dodson, L\" uhrmann, and Mendelson \cite{dodson} when $d=4$, which corresponds to the energy-critical Schr\" odinger equation. More precisely, they proved the local well-posedness of \eqref{intro} when $P=-\Delta$, $d=4$, and $S>\frac{1}{3}$. It is important to note, that instead of using the atomistic framework of \cite{MR3350022}, \cite{dodson} 
used an anisotropic norm denoted by $L_e^{p,q}$, $e\in \R^d$, introduced by Ionescu and Kenig \cite{ionescu1,ionescu2}  to prove well-posedness for the Schr\" odinger map equation. 

By using iterative procedure based on a partial power expansion in terms of the free evolution of the random data, 
 B\' enyi, Oh, and Pocovnicu \cite{bop2} improved the regularity assumption of  \cite{MR3350022} . 
 In a forthcoming work, we adapt their iterative procedure to the functional framework used in this paper.

The only local well-posedness  
result for higher order operators $P$ and randomized data was obtained in \cite{MR4214037} for  
$P= \Delta^2 - \mu \Delta$, $\mu \geq 0$, $d\geq 5$ and $S>\max\big\{\frac{(d-1)(d-4)}{2(d+5)},\frac{d-4}{4} \big\}$. 
  
 The aim of this paper is to extend the framework of \cite{dodson} to arbitrary dimension and more general operator $P$ with real, smooth symbol $p$.
We assume that there is a real $s \geq 2$ such that for all \(\xi\) large enough one has 
\begin{equation} \label{aonth}
|\xi|^{s-1} \lesssim |\partial_{\xi_k} p(\xi)| \lesssim |\xi|^{s-1} \,, \quad 
|\partial^2_{\xi_k\xi_k} p(\xi)| \lesssim |\xi|^{s-2} \,, \quad |\partial^3_{\xi_k\xi_k \xi_k} p(\xi)| \lesssim |\xi|^{s-3},
\end{equation}
for each $k \in \{1, \cdots, d\}$ and
\begin{equation}\label{hots}
|\xi|^{d(s-2)} \lesssim |\textrm{det} H p(\xi)| \lesssim |\xi|^{d(s-2)}
\end{equation}
where $Hp$ is the Hessian of $p$. These conditions are trivially satisfied if $P = \Delta^{s/2} + Q$ for $s \geq 2$ and $Q$ is a lower order operator with real smooth symbol. 
 
For the differentiability $S$ of initial data, we show,  quite interestingly, that there are three different regimes: 
\begin{equation}\label{asos}
S_{\min}(s,d) = (d - s)\times
\begin{cases}
 \frac{(d - 2s + 1)}{2(d - 1)} & s \leq \frac{d + 2}{3} \,, \\
\frac{1}{6} & \frac{d + 2}{3} <  s \leq \frac{d + 1}{2} \,, \\
 \frac{3s - d - 2}{2(d + s - 2)}  & s  >  \frac{d + 1}{2} \,. 
\end{cases}
\end{equation}


Our main result reads as follows:

\begin{thm}
\label{mainthm}
Fix $s \geq 2$, an integer $d>s$, and $S>S_{\min}(s,d)$ with $S_{\min}$ as in \eqref{asos}.  
Let $P$ be an operator whose Fourier symbol $p$ is smooth, real, and satisfies \eqref{aonth} and \eqref{hots}. 
Assume $f\in H_x^S (\R^d)$ and denote  the randomization of $f$ as in \eqref{ricc}. 
Then, for a.e. $\omega \in \Omega$, there exists an open interval $0\in I$ and a unique solution
$$u(t) \in e^{-itP}f^\omega +C(I; \dot{H}_x^{\frac{d-s}{2}} (\R^d))$$
to
\begin{equation}
\label{eqintro}
\begin{cases} (i\partial_t - P) u = \pm |u|^2 u\ on\ I\times \R^d ,\\ u(0)=f^\omega . \end{cases}
\end{equation}
\end{thm}

To compare with existing results, we choose $P=-\Delta$, and in particular $s = 2$. Then, 
\begin{equation*}
S_{\min}(2,d) = (d - 2)\times
\begin{cases}
 \frac{(d - 3)}{2(d - 1)} & d\geq 4 \,, \\
\frac{1}{6} & \ d=3 \,. 
 
\end{cases}
\end{equation*}
Hence, we recover the range of regularities of \cite{dodson} for $d=4$, almost recover \cite{shen2021almost} when $d=3$ (we do not prove the borderline case) and improve that of \cite{MR3350022} for any dimension. We believe our methods can be refined to recover probabilistic estimates for the time of existence of solutions. Notice that the regime $s>  \frac{d + 1}{2}$ does not appear for the Laplacian case $s = 2$. When $P=\Delta^2$, we find
\begin{equation*}
S_{\min}(4,d) = (d - 4)\times
\begin{cases}
 \frac{(d - 7)}{2(d - 1)} & d \geq 10 \,, \\
\frac{1}{6} & 7\leq d<10 \,, \\
 \frac{10 - d }{2(d + 2)}  & 4<d<7 \,. 
\end{cases}
\end{equation*}
Thus, we improve the result of \cite{MR4214037} except when $d=5$. A possible explanation is that our framework (using anisotropic norms) is well adapted when $s\leq \dfrac{d+1}{2}$ 
whereas the framework of \cite{MR3350022,shen2021almost} (using the spaces $U^p$ and $V^p$) is better suited for  $s> \frac{d + 1}{2}$. We also plan to investigate this point in a future work.

We remark that  our assumptions on the operator $P$ allow for sign changing symbol, for example if $P = \Delta^2 \pm \Delta$. Also, our assumptions cover lower order terms, which are not necessarily radial. 






 The proof of Theorem \ref{mainthm} follows closely \cite{dodson}. The main idea is to subtract the free (random) evolution and rewrite \eqref{eqintro} as a forced cubic equation
\begin{equation}\label{eqda}
\begin{cases} (i\partial_t -P) v = \pm |F+v|^2 (F+v),\\ v(t_0)=0 \in \dot{H}_x^{\frac{d-s}{2}} (\R^d)\,,  \end{cases}
\end{equation}
where $F=e^{-itP}f^\omega$ is random and $\dot{H}_x^S$ is a homogeneous Sobolev space. 
Due to randomization, $F$ has almost surely better space-time integrability properties then its deterministic counter-part. The framework can be divided into the following steps. 

\noindent
\textbf{Step 1.} We control the linear evolution $F$ in a Besov-type norm 
$Y$ based on a dyadic decomposition of Fourier space.  On each dyadic anulus, the norm of $Y$ is a weighted (with weights parametrized by $A,B$, and $C$) combination of
 $L_t^q L_x^p$ norms and anisotropic norms $L_{e_l}^{a,b}$ and $L_{e_l}^{b,a}$,  
for carefully chosen positive parameters $q,p,a$, and $b$.  The weights $A, B, C$ are directly linked with the value of $S$, so that 
we want to choose them as small as possible.

\noindent
\textbf{Step 2.} We use a fixed point argument for \(v\) in a space $X$ endowed with a norm based, again, on a dyadic composition of Fourier space. The norm on $X$ is a weighted combination of Strichartz admissible $L_t^r L_x^b$ norms and anisotropic norms.  Thanks to a smoothing and maximal function estimates, we show that if  the right hand side of \eqref{eqda} belongs to $G$ (another Besov-type space), then $v$ belongs to $X$. In addition, if the time is short, then the $X$-norm of $v$ is small. 

\noindent
\textbf{Step 3.} 
The main technical difficulty is to show that if $F \in Y$ and $v \in X$, then the right-hand side of \eqref{eqda} belongs to $G$. Here, we factor the right hand side of the evolution \eqref{eqda} and treat each term separately; each factor is splitted dyadically in Fourier space. At this point, we need a precise choice of parameters $A$, $B$, $C$, which consequently determine the regularity of the initial condition $S$. We show that for small dimensions the strictest condition on $S$ is induced by $F^3$ term. 
As observed in \cite{bop2}, one can hope to absorb $F^3$ to linear evolution, by using the next term in the expansion, that is, by replacing $F$ by 
$F(t) -i \int_0^t e^{-i(t-t^\prime)P} |F(t^\prime)|^2 F(t^\prime) dt^\prime +C(I; \dot{H}_x^{\alpha} (\R^d))$, for some $\alpha>0$ and $f\in H^{S_1}_x (\R^d)$ with $S_1<S$. 
However,  in high dimensions,  which corresponds to a regime where the nonlinearity is `very supercritical', it seems that the most restrictive nonlinear term is $Fv^2$ or $F^2 v$, and further expansion is less obvious. 

The organization of the paper is as follows: in Section \ref{prelim}, we recall classical facts about the Littlewood-Paley (dyadic Fourier space) projections and introduce the anisotropic norms $L^{p,q}_e$. We then prove maximal function and local smoothing estimates for the free evolution operator. In Section \ref{sec:lin}, we prove a linear Strichartz-type estimate for functions belonging to the space $X(I)$, which is a mixture of Strichartz norms and $L^{p,q}_e$ norms. Section \ref{sec:nonlin} contains trilinear estimates that control interactions coming from the cubic nonlinearity. Finally, in Section \ref{sec:random}, we recall probabilistic estimates for randomized initial data. Then, we give a localized maximal function estimate which allows us to prove that the free evolution of $f^\omega$ belongs to $Y(\R)$ when $S$ is large enough. Finally, we establish Theorem \ref{mainthm} by using a fixed point argument. 

Theorem \ref{mainthm} requires \(d > s\) which we  implicitly assume  henceforth and we might not state it explicitly in the statements that below. 

\subsection{Notation}
\begin{itemize}
\item We denote $\mathcal{F}(f)(\xi) = \hat{f}(\xi)=\int_{\R^{d}}f(x) e^{-i \xi x} d x$ the Fourier transform of the function $f$. The dimension \(d\) is extrapolated from context.
\item For functions $a, b$  we write $a\lesssim b$ if there exists a constant $C>0$ such that $a\leq Cb$.
\item We use the notation $a\approx b$, if $a\lesssim b$ and $b\lesssim a$.
\item We denote the ball of radius \(r\) and center \(x\) as \(B_{r}(x)\); if \(x=0\) we simply write \(B_{r}\). The dimension of the ball is to be understood from the context unless explicitly written, for example 
\(B_{r}^{d}\).
\item For $p\geq 1$, $p^\prime$ stands for the dual of $p$, that is,  $\frac{1}{p^\prime}+\frac{1}{p}=1$.
\item For $\alpha >0$,  $\dot{H}^\alpha (\R^d)$ denotes the homogeneous Sobolev space endowed with the semi-norm
$$\|u\|_{\dot{H}^\alpha (\R^d)}^2 = \int_{\R^d} |(-\Delta)^{\alpha /2} u(x)|^2 dx.$$
\item We denote by $\langle .\rangle$ the Japanese bracket: $\langle N\rangle = (1+N^2)^{\frac{1}{2}}$.
\end{itemize}


\section{Generalities}\label{prelim}
In this section, we recall Littlewood-Paley projections, Strichartz estimates and anisotropic norms $L^{p,q}_e$ introduced by Ionescu and Kenig \cite{ionescu1,ionescu2}, the usage of which is the main novelty of this paper. Then, we prove a maximal function estimate \eqref{IK1} and a local smoothing estimate \eqref{IK3} for $L^{p,q}_e$. The estimate \eqref{IK3} in particular allow us to `gain' $\frac{s-1}{2}$ 
derivatives in our estimates of the nonlinear terms. 

We begin by defining the Littlewood-Paley projections $P_N$ for $N\in 2^{\Z}$. We henceforth fix a cutoff function $\varphi \in C_{c}^{\infty} (\R)$, that is, 
a function such that $\varphi (\xi)=1$ for $|\xi|\leq 1$ and $\varphi (\xi)=0$ for $|\xi|>1+2^{-100}$. We set
\begin{equation}\label{dyadic-anulus-bump}
\Delta_{0}\varphi(\xi)=\varphi(2\xi)-\varphi(\xi)\qquad\Delta_{N}\varphi(\xi)=\Delta_{0}\varphi(\xi/N) .
\end{equation}
The function \(\Delta_{N}\varphi(\xi)\) is supported on \(N/2<|\xi|<N(1+2^{-100})\) and is equal to \(1\) when \((1+2^{-100})N/2<|\xi|<N\). We set
\begin{equation}\label{LPproj}
\widehat{P_N f} (\xi)= \Delta_{N}\varphi(|\xi|)\hat{f} (\xi).
\end{equation} 
Note that this projection is different than the one introduced in \eqref{proj1}. Next, we recall the classical Bernstein estimates.

\begin{lem}
\label{bern}
Fix $N\in 2^{\Z}$. For any $1\leq r_1 \leq r_2 \leq \infty$ and any $s\geq 0$, it holds
$$\|P_N f\|_{L_x^{r_2} (\R^d)}\lesssim N^{d(1/r_1 - 1/r_2)} \|P_N f\|_{L_x^{r_1}(\R^d)},$$
and
$$\||\nabla|^{\pm s} P_N f \|_{L_x^{r_1} (\R^d)} \approx N^{\pm s} \|P_N f\|_{L_x^{r_1} (\R^d)}.$$
\end{lem}
Since we use an anisotropic norm, we also need to define anisotropic Littlewood-Paley projections. Let $\{e_1 ,\ldots, e_d\}$ be an orthonormal basis of $\R^d$
and let $\phi \in C^\infty_c(\R)$ be a smooth function supported on \(B_{R}\setminus B_{R^{-1}}\) for some \(R\gg 1\). For any $N\in 2^{\Z}$ and any $l=1,\ldots ,d$, we define
\begin{equation} \label{LP-anisotropic}
\widehat{P_{N,e_l} f} (\xi) = \Delta_{N}\phi(\xi\cdot e_l)\hat{f} (\xi) \,.
\end{equation}
Note that the functions $\phi$ and $\varphi$ are different since they have, for example, different domains.
 By support considerations in Fourier space, for already fixed $\varphi$, we can choose $\phi$ such that for any \( N \in 2^{\Z}\) it holds that
\begin{equation}
\label{e1}
\prod_{l=1}^{d} \Big( 1-  P_{N,e_l}\Big) P_{N} =0.
\end{equation}


Next, we recall the Strichartz estimates for general self-adjoint operator $P$ of order $s$ with constant coefficients. We say that a pair $(q,r)$ is admissible (for $P$) if
\begin{equation}
\label{defadmissible}
\frac{s}{q} +\frac{d}{r}=\frac{d}{2},\ 
\begin{cases}r\in [2, \frac{2d}{d-s}) & \textrm{if } \ d> s, \\  
r\in [2,\infty ) & \textrm{if } \ d=s,\\ 
r\in [2,\infty] &  \textrm{if } \ d <  s. 
\end{cases}
\end{equation}




\begin{lem}[\cite{MR2787438,MR3852677,MR1151250,MR1646048}]
\label{strichartz}
Let $d\geq 1$, $\mu \geq 0$. Assume that $(q,r)$ and $(\tilde{q},\tilde{r})$ are admissible. Then, there exists $t_0>0$ such that, for any $I\subset \R$ with $|I|\leq t_0$, one has
\begin{equation}
\label{strichartze1}
\|e^{-it P} f\|_{L_t^q L_x^r (I\times \R^d)} \lesssim \|f\|_{L^2 (\R^d )},
\end{equation}
and
$$\left\|\int_I e^{is P  } f(s,.) ds\right\|_{L_x^2 (\R^d)}\lesssim \|f\|_{L_t^{q^\prime} L_x^{r^\prime} (I\times \R^d)}. $$
Assuming that $0\in I$, we also have
\begin{equation}
\label{strichartze3}
\left\|\int_I e^{-i(t-s)P  } f(s,.) ds \right\|_{L_t^q L_x^r (I\times \R^d)}\lesssim \|f\|_{L_t^{\tilde{q}^\prime} L_x^{\tilde{r}^\prime} (I\times \R^d)} . 
\end{equation}
\end{lem}
\begin{rmq}
Notice that the evolution group \(e^{-itP}\) commutes with projections \(P_{N}\), \(P_{N,e_{l}}\) for any \(l\in\{1,\ldots,d\}\) and any \(N\in2^{\Z}\), and with \(Q_{n}\) for \(n\in\Z^{d}\). Thus the bounds above also hold with \(f\) replaced by \(P_{N}f\), \(P_{N,e_{l}}f\), and \(Q_{n}f\) on both sides of the inequality.
\end{rmq}

In the rest of the paper, we assume that all time intervals have length less than the $t_0$ given in Proposition \ref{strichartz}, and therefore the Strichartz estimates hold. 
Since we are only interested in local existence, this assumption does not influence our main results.

 To introduce the anisotropic norms, decompose $x\in \R^d$  for any $l \in \{1, \dots, d\}$ as
$$
x=x_l e_l+\displaystyle\sum_{i=1,\ i\neq l}^d x_i e_i =: x_l e_l + x^\prime_l 
$$
and, if there is no possible confusion, we write $x^\prime := x_l^\prime$.  
Fix $I \subset \R$ and $l \in \{1, \dots, d\}$,  and for $1 \leq p,q<\infty$ define
$$\|f\|_{L_{e_l}^{p,q} (I\times \R^d )} = \Bigg(\int_{\R}  \Bigg(\int_I \int_{\R^{d-1}} |f(t,x_le_{l}+x_l')|^q dx_l' dt\Bigg)^{\frac{p}{q}} dx_l \Bigg)^{1/p} \,,$$
where $f : I \times \R^d \to \mathbb{C}$ is such that the right hand side is finite.
When $p=\infty$ or $q=\infty$, we use the standard modifications by supremum norm. 
Next, we establish a maximal and a local smoothing estimates for our anisotropic norm. 



\begin{lem}
\label{1lemIK}
Let \(P\) satisfy the conditions \eqref{aonth} and \eqref{hots}. Given an interval $I\subset \R$ we have, for $N$ large enough,
\begin{equation}\label{IK1}
\| e^{-it P }  P_N f\|_{L_{e_l}^{2,\infty} (I\times \R^d)}\lesssim N^{X_1} \|P_N f\|_{L_x^2 (\R^d )},
\end{equation}
and
\begin{equation}
\label{IK3}
\|e^{-it P}  P_{N, e_l }  P_N f \|_{L_{e_l}^{\infty ,2} (I \times \R^d)} \lesssim N^{X_2} \|P_N f\|_{L^2 (\R^d )},
\end{equation}
with
$$X_1 =\dfrac{d-1}{2} ,\qquad X_2= -\dfrac{s-1}{2}.$$
\end{lem}
\begin{rmq}
Notice that the constant $X_1$ in \eqref{IK1} does not depend on $P$ but only on the dimension $d$, whereas $X_2$ only depends on the order of $P$ but not on $d$.
\end{rmq}

\begin{proof}
First, we prove \eqref{IK1} and without loss of generality we assume $l=1$ and set $e = e_1$. We use a \(TT^{*}\) argument for $T$ defined as 
\begin{equation*}
T f(x)=\frac{1}{(2\pi)^{d}} \int_{\R^{d}} e^{\xi x - i t p(\xi)} h_{N}(\xi)\hat{f}(\xi)d \xi,
\end{equation*}
where $p$ is the symbol of the operator $P$, that is, $p(\xi)=\hat{P}(\xi)$, and
\begin{equation*}
h_{N}(\xi)=\phi\Big(\frac{|\xi)|}{2N}\Big)-\phi\Big(\frac{4|\xi|}{N}\Big)=\sum_{k=-1}^{1}\Delta_{2^{k}N}\phi(|\xi|)
\end{equation*}
so that \(h_{N}(\xi)\Delta_{N}\phi(|(\xi)|)=\Delta_{N}\phi(|(\xi)|)\). Our claim follows from \(\|Tf\|_{L_{e}^{2,\infty} (I\times \R^d)}\lesssim N^{X_{1}} \|f\|_{L^{2}_{x}(\R^{d})}\), 
or equivalently \(\|T^{*}g\|_{L^{2}_{x}(\R^{d})}^{2}\lesssim N^{2X_{1}}\|g\|_{L_{e}^{2,1} (I\times \R^d)}^{2}\). By  \(TT^{*}\) argument  it suffices to show 
\begin{equation}\label{ttsa}
\|TT^{*}g\|_{L_{e}^{2,\infty} (I\times \R^d)}\lesssim N^{2X_{1}}\|g\|_{L_{e_l}^{2,1} (I\times \R^d)}.
\end{equation}
Indeed, if \eqref{ttsa} holds, then
\begin{align*}
\|T^{*}g\|_{L^{2}_{x}(\R^{d})}^{2} &= \langle T^{*}g, T^{*}g \rangle = \langle TT^{*}g, g \rangle \lesssim \|TT^{*}g\|_{L^{2, \infty}_{e}} \|g\|_{L_{e}^{2,1}}
\\
&
\lesssim   N^{2X_{1}}\|g\|_{L_{e_l}^{2,1} (I\times \R^d)}^2 \,,
\end{align*}
as desired. 
Direct computations show that
\begin{equation*}
TT^{*}g (t, x)= \int_{\R\times\R^{d}} K_{N}(t-s,x -y) g(s, y)  ds dy
\end{equation*}
where 
\begin{equation*}
 K_{N}(t,x) =\frac{1}{(2\pi)^{2d}}\int_{\R^{d}} e^{i x \xi} e^{-it p (\xi) }  h_{N}(\xi)d\xi.
\end{equation*}
It is thus sufficient prove
$$
\left\| K_{N}\right\|_{L^{1, \infty}_e (\R\times\R^{d})} \lesssim N^{2X_1}.
$$
Let us start by recording that 
$$| K_{N}(t,x_1 ,x')|\lesssim  \min (N^d , |t|^{-\frac{d}{s}}). $$
Indeed, the bound by $N^d$ follows by exchanging the absolute value and the integral and by noticing that \(h_{N}\) is supported on a set of measure \(N^{d}\). The bound by $|t|^{-\frac{d}{s}}$ follows from  \cite[Theorem 3.1]{MR3710698}.  Note that the assumptions of \cite[Theorem 3.1]{MR3710698} are satisfied by \eqref{aonth} and \eqref{hots} on the support of $h_N(\xi)$. 
To gain decay in \(x_{1}\) we exploit the oscillations of the phase factor by integating by parts twice:
\begin{align*}
K_{N}(t,x_1 ,x') &=-i\int_{\R \times \R^{d-1}} \frac{
\partial_{\xi_1} e^{i\big(x_1 \xi_1 +x' \cdot \xi' -t p (\xi)\big)}}{x_1 - t\partial_{\xi_{1}}p(\xi_{1}, \xi')}    h_N(\xi)d\xi_1 d\xi'
\\ 
& = i\int_{\R \times \R^{d-1}}  e^{i\big(x_1 \xi_1 +x' \cdot \xi' -t p (\xi)\big)} \partial_{\xi_1} \Big[\frac{h_N(\xi)}{x_1 - t\partial_{\xi_{1}}p (\xi)} \Big] d\xi_1 d\xi^\prime
\\ 
& =
\begin{aligned}[t]
&-\int_{\R \times \R^{d-1}}  e^{i\big(x_1 \xi_1 +x' \cdot \xi' -t p (\xi)\big)}
\\ & 
\qquad\times\partial_{\xi_{1}} \left[\frac{1}{x_1 - t\partial_{\xi_{1}}p (\xi)} \partial_{\xi_{1}}\Big[\frac{h_N(\xi)}{x_1 - t\partial_{\xi_{1}}p (\xi)} \Big]\right] d\xi_1 d\xi'.
\end{aligned}
\end{align*}
The scaling implies for all positive integers $k$ that
\begin{equation*}
\big|(\partial_{\xi_1})^{k} h_N(\xi)\big| \lesssim N^{-k}1_{|\xi|\lesssim N} 
\end{equation*}
To control the denominator we restrict ourselves to the regime  $|x_1| \gtrsim N^{s-1} |t|$. Since the term \(h_{N}(\xi)\) vanishes unless \(|\xi_{1}|\lesssim N|t|\) 
and by \eqref{aonth} we have  $|\partial_{\xi_{1}}p(\xi_{1},\xi')| \lesssim \big|(\xi_{1},\xi')\big|^{s-1}$, then
\begin{equation*}
\frac{1}{|x_1 - t\partial_{\xi_{1}}p(\xi_{1},\xi')|} \lesssim\frac{1}{|x_1|}
\end{equation*}
Furthermore,  from \eqref{aonth} it follows that
$$
\Bigg|\partial_{\xi_1}  \Big(\frac{1}{x_1 - t\partial_{\xi_{1}}p(\xi_1, \xi')}\Big)\Bigg|
=
 \Bigg|\frac{t \partial_{\xi_{1},\xi_1}^{2}p(\xi_1, \xi')) }{(x_1 - t\partial_{\xi_{1}}p(\xi_1, \xi'))^2}\Bigg| \lesssim 
\frac{ |t| \,\big|(\xi_1, \xi')\big|^{s-2} }{|x_1|^{2} } \lesssim 
\frac{1}{N |x_1|}
$$
and similarly 
$$
\Bigg|\partial^2_{\xi_1,\xi_1}  \Big(\frac{1}{x_1 - t\partial_{\xi_{1}}p(\xi_1, \xi')}\Big)\Bigg|
\lesssim
\frac{1}{N^2 |x_1|^2}.
$$
Thus for $|x_1| \gtrsim N^{s-1} |t|$ one has
$$|K_{N}(t,x_1 ,x')|\lesssim 
\frac{1}{\langle N x_1 \rangle^{2}}  \int_{|\xi|\lesssim2^{N}} d\xi \lesssim
  \frac{N^d}{\langle N x_1 \rangle^{2}}.$$
The above estimates combined yield
\begin{align*}
\sup_{t\in \R , x' \in \R^{d-1}} |K_{N} (t,x_1, x')| &\lesssim   \frac{N^d}{\langle N |x_1|\rangle^{2}} +\min\Big(N^d , 1_{\{|x_1| \lesssim N^{s-1} |t|\} }  |t|^{-\frac{d}{s}}\Big)\\
& \lesssim   \frac{N^d}{\langle N x_1\rangle^{2}} + N^d \min \left(1,|Nx_1|^{-\frac{d}{s} } \right)\\
&\lesssim N^{d} \bigg( \frac{1}{\langle N x_1\rangle^{2}}  +\frac{1}{\langle N x_1\rangle^{d/s}} \bigg).
\end{align*}
An integration in $x_1$ implies
$$\left|\int_{\R} \sup_{t\in \R , x^\prime \in \R^{d-1}} |K_{N} (t,x_1, x')| dx_1 \right| \lesssim N^{d-1} 
$$
and \eqref{IK1} follows. 

Next, let us prove \eqref{IK3}. 
Let us fix \(x_{1}\) and estimate
\begin{equation*}
I_{x_{1}}=\int_\R \int_{\R^{d-1}} \left|\int_\R \int_{\R^{d-1}} e^{i\big(x_1 \xi_1 +i x'\cdot\xi' -tp(\xi_{1},\xi')\big)} \widehat{P_N f}(\xi_1 ,\xi') \Delta_{N}\phi(\xi_{1}) d\xi' d\xi_1\right|^2 dx' dt, 
\end{equation*}
that appears on the RHS of \eqref{IK3}. Plancherel's identity in the variable \(x'\) yields
$$I_{x_{1}}\approx \int_\R \int_{\R^{d-1}} \left|\int_\R e^{i\big(x_1 \xi_1 -tp(\xi_{1},\xi')\big)}\widehat{P_N f}(\xi_1 ,\xi') \Delta_{N}\phi(\xi_{1}) d\xi_1\right| ^2 d\xi' dt. $$
Let us restrict our analysis to 
$$I_{x_{1}}^{+}:= \int_\R \int_{\R^{d-1}} \left|\int_{0}^{+\infty} e^{i\big(x_1 \xi_1 -tp(\xi_{1},\xi')\big)}\widehat{P_N f}(\xi_1 ,\xi') \Delta_{N}\phi(\xi_{1}) d\xi_1\right| ^2 d\xi' dt. $$
The term \(I_{x_{1}}^{-}\), defined analogously, is estimated symmetrically.
If \(N\) is sufficiently large, the integrand of \(I_{x_{1}}^{+}\) vanishes unless \(\xi_{1}\approx N\).  In this regime, the function $\xi_1 \mapsto p_{\xi'}(\xi_1) := p(\xi_1 ,\xi')$ is invertible on a neighborhood of the support of \(\Delta_{N}\phi(\xi_{1})\) since \(\partial_{\xi_{1}}p(\xi_{1},\xi')\) is non-vanishing and thus does not change sign. Then, the change of variables $\theta=p_{\xi^\prime}(\xi_1)$ implies
\begin{align*}
I_{x_{1}}^{+}= \int_\R \int_{\R^{d-1}}\bigg|\int_{p_{\xi'}([N/2,2N])} \hspace{-3em} &e^{i(x_1 p_{\xi'}^{-1}(\theta)-t\theta) }
\widehat{P_N f}(p_{\xi'}^{-1}(\theta),\xi')
\\ & \times \Delta_{N}\phi\big(p_{\xi'}^{-1}(\theta) \big)
\frac{d}{d \theta}(p_{\xi' }^{-1} (\theta)) d\theta \bigg|^2 d\xi' dt \,,
\end{align*}
and from Plancherel identity in the variable \(t\), it follows
$$
I_{x_{1}}^{+}\approx 
\int_{p_{\xi'}([\frac{N}{2},2N])} \int_{\R^{d-1}} \left|  e^{ix_1 p_{\xi^\prime}^{-1}(\theta)  }  \widehat{P_N f}(p_{\xi'}^{-1}(\theta),\xi')  \Delta_{N}\phi\big(p_{\xi'}^{-1}(\theta) \big) 
\frac{d}{d \theta}\big(p_{\xi^\prime }^{-1} (\theta)\big) \right|^2  d\xi' d\theta. $$
Returning to the original variable \(\xi_{1}\) with \(\theta=p_{\xi'}(\xi_{1})\) and using \(|\frac{\partial}{\partial \theta}(p_{\xi^\prime }^{-1} (\theta)) | = \frac{1}{|\frac{d}{d \xi_1}(p_{\xi'}(\xi_{1})) |}\) we obtain 
\begin{equation*}
I_{x_{1}}^{+}\lesssim\int_{N/2}^{2N}\int_{\R^{d-1}}\Big| \widehat{P_N f}(\xi_{1},\xi')  \Delta_{N}\phi(\xi_{1}) \Big|^{2}\frac{1}{|\frac{d}{d \xi_1}(p_{\xi'}(\xi_{1})) |}d\xi'd\xi_{1}.
\end{equation*}
By \eqref{aonth},   $| \frac{1}{|\frac{d}{d \xi_1}(p_{\xi'}(\xi_{1})) |} \lesssim \frac{1}{|\xi_1|^{s-1}}$ when $\xi_1 \approx N \gg 1$, and therefore 
\begin{align*}
I_{x_{1}}
&\lesssim \int_{\R}\int_{\R^{d-1}}\Big| \widehat{P_N f}(\xi_{1},\xi')  \Delta_{N}\phi(\xi_{1}) \Big|^{2} \frac{1}{N^{(s-1)}}d\xi'd\xi_{1}.
\\ &=\frac{1}{N^{s-1}}\|P_{N}P_{N,e_{1}}f\|_{L^{2}(\R^{d})}\leq \frac{1}{N^{s-1}}\|P_{N}f\|_{L^{2}(\R^{d})}.
\end{align*}
This concludes the proof of \eqref{IK3}.
\end{proof}

Next, we show bounds that are obtained by interpolating the inequalities \eqref{IK1} and \eqref{IK3} with the Strichartz estimates \eqref{strichartze1}.

\begin{lem}
\label{lemmaIK}
Fix $I\subset \R$, $2\leq p,q\leq \infty$ such that $1= \frac{2}{p} +\frac{2}{q}$, $N\in 2^\Z$ and $l\in \{1,\ldots ,d\}$. Then,  for any $l\in \{1,\ldots, d\}$  we have:
\begin{enumerate}
\item If $q \geq p$, then for any  $f \in L^2(\R^d)$
\begin{equation}
\label{lemmaIKe1}
\|e^{-it P} P_N f\|_{L_{e_l}^{p,q} (I \times \R^d )} \lesssim N^{\alpha (p,q)} \|P_N f\|_{L^2 (\R^d )} ,
\end{equation}
and for any $h \in L_{e_l}^{p^\prime ,q^\prime}(I\times \R^d )$
\begin{equation}
\label{lemmaIKe1bis}
\left\|\int_I e^{is P} P_N h(s,\cdot) ds \right\|_{L^2_{x} (\R^d )} \lesssim N^{\alpha (p,q)} \|P_N h\|_{L_{e_l}^{p^\prime ,q^\prime} (I\times \R^d )},
\end{equation}
 with 
$$\alpha (p,q)= \dfrac{d+s-2}{p}+\frac{1-s}{2}.$$
\item If $q<p$, then for any  $f \in L^2(\R^d)$
\begin{equation}
\label{lemmaIKe2}
\|e^{-it P} P_{N, e_{l}} P_N f\|_{L_{e_l}^{p,q} (I\times \R^d )} \lesssim N^{\beta (p,q)} \|P_N f\|_{L^2 (\R^d )} ,
\end{equation}
and for any $h \in L_{e_l}^{p^\prime ,q^\prime}(I\times \R^d )$
\begin{equation}
\label{lemmaIKe2bis}
\left\| \int_I e^{is P} P_{N, e_l} P_N h(s, \cdot) ds\right \|_{L^2 (\R^d )} \lesssim N^{\beta (p,q)} \|P_N h\|_{L_{e_l}^{p^\prime ,q^\prime}(I\times \R^d )} ,
\end{equation}
with 
\begin{equation}
\label{beta-lemmaIK}
\beta (p,q)=\alpha (p,q)= \dfrac{d+s-2}{p}+\dfrac{1-s}{2}.
\end{equation}
\end{enumerate}
\end{lem}

\begin{proof}
Without loss of generality let us assume \(l=1\) and set $e = e_1$.
First,  notice that \eqref{lemmaIKe1bis} and \eqref{lemmaIKe2bis} are respectively duals of \eqref{lemmaIKe1} and \eqref{lemmaIKe2}. 
For any fixed \(x_{1}\in\R\) the logarithmic convexity of Lebesgue spaces implies that for any $q \in [2, 4]$,  any $h\in L^2 (I\times\R^d)$, and almost every $x_1 \in \R$ that 
\begin{equation}\label{ilss}
\|h(\cdot,x_{1},\cdot)\|_{L^q_{t}L^{q}_{x'} (I\times \R^{d-1})}\lesssim \|h(\cdot,x_{1},\cdot)\|_{L^2_{t}L^{2}_{x'} (I\times \R^{d-1})}^{\frac{4-q}{q}} \|h(\cdot,x_{1},\cdot)\|_{L^4_{t}L^{4}_{x'} (I\times \R^{d-1})}^{\frac{2(q-2)}{q}}.
\end{equation}
Since $1= \frac{2}{p} +\frac{2}{q}$, if $p > q$ then \(q\in[2,4]\) and after integration of \eqref{ilss} we have
\begin{align*}
 \big\|e^{-it P }  P_{N,e } P_N  f\big\|_{L^{p,q}_{e} (I\times\R^{d})}^{p} 
 &\lesssim
\int_{\R} \big\|e^{-it P } P_{N,e } P_N  f(x_{1})\big\|_{L^{2}_{t}L^{2}_{x'} (I\times \R^{d-1})}^{p\frac{4-q}{q}}
\\ 
&\qquad  \times\big\|e^{-it P }  P_{N,e } P_N  f(x_{1})\big\|_{L^{4}_{t}L^{4}_{x'} (I \times \R^{d-1})}^{p\frac{2(q-2)}{q}} dx_1
\\ 
&\lesssim \|e^{-it P }   P_{N,e } P_N f\|_{L_{e}^{\infty, 2} (I\times \R^{d})}^{p\frac{4-q}{q}}
 \|e^{-it P }  P_N f  \|_{L_{e_{1}}^{4,4}(I\times \R^d)}^4
\end{align*}
A bound on the first factor on the right hand side is provided by the anisotropic estimate \eqref{IK3}. 
A combination of Lemma \ref{bern}  with $(r_1, r_2) = (\frac{4d}{2d - s}, 4)$ and Strichartz estimate \eqref{strichartze1} gives us
\begin{equation}\label{IK2}
\begin{aligned}
\|e^{-it P }  P_N f  \|_{L_{e_{1}}^{4,4}(I\times \R^d)}  &= \|e^{-it P }  P_N f\|_{L_t^4 L_x^4 (I \times \R^d)}
\\ & \lesssim   N^{\frac{d -s}{4}}\|e^{-it P }  P_N f\|_{L_t^4 L_x^{4d /(2d -s)} (I \times \R^d)}
\\ &\lesssim   N^{\frac{d -s}{4}} \|P_N f\|_{L^2 (\R^d )}.
\end{aligned}
\end{equation}
Combining the two estimates yields
\begin{equation*}
\big\|e^{-it P } P_{N,e_{1} } P_N  f\big\|_{L^{p,q}_{e_{1}} (I\times\R^{d})}^{p} \lesssim N^{X_2 \frac{(4-q)p}{q}+d -s}  \|P_N f\|_{L^2 (\R^d )}^p 
\end{equation*}
and \eqref{lemmaIKe2} follows. 

Next, we prove  \eqref{lemmaIKe1}. This time, we interpolate between the frequency limited Strichartz estimate \eqref{IK2} and the anisotropic estimate \eqref{IK1}. Assuming that $q \geq p$, then it holds that $q \geq 4$ and for any $h\in L^\infty (\R^d )$ and almost any fixed \(x_{1}\) we have
$$\|h (\cdot,x_{1},\cdot)\|_{L^q_{t}L^{q}_{x'} (I\times \R^{d-1})} \lesssim \|h(\cdot,x_{1},\cdot) \|_{L^\infty_{t}L^{\infty}_{x'} (I\times \R^{d-1} )}^{1-4/q} \|h(\cdot,x_{1},\cdot)\|_{L^4_{t}L^{4}_{x'} (I\times \R^{d-1} )}^{4/q}.$$
Then,  H\" older inequality implies
\begin{align*}
\big\|e^{-it P } P_N  f\big\|_{L^{p,q}_{e_{1}} (I\times\R^{d})}^{p}
&= \int_{\R} \big\|e^{-it P }  P_N f(x_{1}) \big\|_{L^q_{t}L^{q}_{x'} (I \times \R^{d-1})}^p dx_1
\\ 
&\lesssim \left(\int_\R \|e^{-it P }  P_N f(x_{1}) \|_{L^\infty_{t}L^{\infty}_{x'} (I\times \R^{d-1})}^{(1- \frac{4}{q})p \beta'} dx_{1} \right)^{\frac{1}{\beta'}}
\\ &\qquad \times \left(\int_{\R} \|e^{-it P }  P_N f(x_{1}) \|_{L^4_{t}L^{4}_{x'} (I\times \R^{d-1})}^{\frac{4p \beta}{q}}) dx_{1} \right)^{\frac{1}{\beta}},
\end{align*}
where we choose $\beta=\frac{q}{p}$ and $\beta' = \frac{q}{q-p}$ to satisfy \(\beta,\beta'\geq1\) and \(\frac{1}{\beta}+\frac{1}{\beta'}=1\). From the condition $1= \frac{2}{q} +\frac{2}{p}$ and the inequalities \eqref{IK1} and \eqref{IK2} we obtain
\begin{multline*}
\int_\R \|e^{-it P}  P_N f(x_{1}) \|_{L^q_{t}L^{q}_{x'} (I\times \R^{d-1})}^p dx_1 \\
\begin{aligned}
& \lesssim \|e^{-it P }  P_N f \|_{L^{2,\infty}_{e_{1}} (I\times \R^d )}^{2(1-\frac{p}{q})} \|e^{-it P }  P_N f \|_{L^4_{t}L^{4}_{x} (I\times \R^d)}^{4\frac{ p}{q}} \\
&\lesssim N^{2X_1 (1-\frac{p}{q}) +(d-s) \frac{p}{q} } \|P_N f\|_{L^2 (\R^d)}^p ,
\end{aligned}
\end{multline*}
which concludes the proof.
\end{proof}


\section{Linear estimates}\label{sec:lin}
This section is devoted to the proof of linear Strichartz type estimates for solutions belonging to an `anisotropic Besov-type space' $X(I)$, $I\subset \R$ (see Proposition \ref{mainproplinearest}). 
The norm of $X(I)$ is based on a decomposition of frequency space into dyadic anuli
$$\|v\|_{X(I)}=\left(\sum_{N\in 2^{\Z}} \|P_N v\|^2_{X_N (I)}\right)^{\frac{1}{2}} \,,$$
where the norm $X_N (I)$ is  defined as 
\begin{equation} \label{dfxn}
\|P_N v\|_{X_N (I)}= N^\alpha\Big( \sum_{i=1}^{3} \|P_N v\|_{L_t^{q_i} L_x^{r_i} (I\times \R^d)} +N^{-\alpha(\tilde{p},\tilde{q}) } \sum_{l=1}^d \|P_N v \|_{L_{e_l}^{\tilde{p},\tilde{q} } (I\times \R^d)}\Big),
\end{equation}
where $(q_i ,r_i)$, $i=1,2,3$, are admissible pairs as in \eqref{defadmissible}, $1= \frac{2}{\tilde{p}} +\frac{2}{\tilde{q}}$ with $\alpha(\tilde{p},\tilde{q}) $ as in Lemma \ref{lemmaIK}, and 
$\alpha>0$ is a constant defined below as $\alpha = \frac{d - s}{2}$. We  prove that if $v$ is a solution to
$$\begin{cases}(i\partial_t - P)v =h, \ \textrm{on } \ I\times \R^d, \\ v(t_0)=0,   \end{cases}$$
then
$$\|P_N v\|_{X_N (I)} \lesssim \|P_N h\|_{G_N (I)}, $$
where the norm $G_N (I)$ is given as 
$$\|v\|_{G(I)}=\left(\sum_{N\in 2^{\Z}} \|P_N v\|^2_{G_N (I)}\right)^{\frac{1}{2}}.$$
with
$$\|P_N h\|_{G_N (I)} =\inf_{P_N h = h_1 +h_2} 
\Big\{N^\alpha \|h_1 \|_{L_t^{1} L_x^{2} (I\times \R^d)} +\sum_{l=1}^d N^{\beta(A,B) +\alpha} \| h_2\|_{L_{e_l}^{A^\prime ,B^\prime} (I\times \R^d )} \Big\},
$$
where $1= \frac{2}{B} +\frac{2}{ A}$ and $\beta(A, B)$ is given by \eqref{beta-lemmaIK}. 
Notice that $(A', B') = (1,2)$ is a pair of exponents that is dual to the pair $(A, B) = (\infty ,2)$, that satisfies the constraints of Lemma \ref{lemmaIK}.

We begin with the following lemma.


\begin{lem}
\label{lemlinsecond}
Fix $l \in \{1, \cdots, d\}$, $N\in 2^\Z$, and two time intervals $I,J \subset \R$. Then, for any andmissible pair $(q,r)$ in the sense of \eqref{defadmissible} and any $A,B$ 
such that $1= \frac{2}{B} +\frac{2}{ A}$, and $h : I \cup J \times \R^d \to \R$ we have
$$\left \|\int_J e^{-i(t-s) P} P_N h(s) ds\right\|_{L_t^q L_x^r (I\times \R^d )} \lesssim  \sum_{l=1}^d N^{\beta (A,B)} \|P_N h\|_{L_{e_l}^{A^\prime ,B^\prime} (J\times \R^d )} .$$
Moreover, if $(\tilde{p},\tilde{q}) $ is such that $1= \frac{2}{\tilde{p}} +\frac{2}{\tilde{q}}$, then
$$ \sum_{l=1}^d\left\|\int_J e^{-i(t-s) P} P_N h(s) ds \right\|_{L_{e_l}^{\tilde{p},\tilde{q}} (I\times \R^d)}\lesssim N^{\alpha(\tilde{p} ,\tilde{q})} \sum_{l=1}^d N^{\beta (A,B)} \|P_N h\|_{L_{e_l}^{A^\prime ,B^\prime} (J\times \R^d )}. $$
\end{lem}


\begin{proof}
To simplify the notation, we do not indicate the dependence of $h$ on $x$. 
By the Strichartz estimate \eqref{strichartze1}, if $(q,r)$ is admissible, then
$$\left\|\int_J e^{-i(t-s) P} P_N h(s) ds\right\|_{L_t^q L_x^r (I\times \R^d)} \lesssim \left\|\int_J e^{is P } P_N h(s) ds\right\|_{L^2 (\R^d)} $$
and the first assertion follows from \eqref{lemmaIKe1bis} if $A \geq B$.
Next, by \eqref{e1}, we have
\begin{align*}
P_N 
= P_{N} \sum_{k = 1}^d P_{N, e_k} \prod_{j = 1}^{k - 1}(1 - P_{N, e_j}) \,.
\end{align*}
Since $P_{2^{k'}N,e_j}$ are bounded operators on $L^2_x (\R^d )$, we obtain  
$$
\left\|\int_J e^{is P } P_N h(s) ds\right\|_{L^2 (\R^d )} \lesssim \sum^{d}_{l=1}\left\|\int_J e^{is P} P_{N,e_l} P_N h(s) ds\right\|_{L^2 (\R^d )}.
$$
 Estimate \eqref{lemmaIKe2bis} implies that for $ A < B$, that 
$$\sum_{l=1}^d  \left\|\int_J e^{is P} P_{N,e_l} P_N h(s) ds \right\|_{L^2 (\R^d )} \lesssim \sum_{l=1}^d N^{\beta (A,B)} \|P_N h\|_{L_{e_l}^{A^\prime ,B^\prime} (J\times \R^d )}$$
and the first assertion follows. 


To prove the second statement assume $1= \frac{2}{\tilde{p}} +\frac{2}{\tilde{q}}$ and $\tilde{q} < \tilde{p}$. Then, by 
\eqref{lemmaIKe2}, 
\begin{equation}
\label{line1}
\left\|\int_J e^{-i(t-s) P} P_N h(s) ds \right\|_{L_{e_l}^{\tilde p, \tilde q} (I\times \R^d)}\lesssim N^{\alpha ( \tilde{p},\tilde q)}  \left\|\int_J e^{is P } P_N h(s) ds \right \|_{L^2 (\R^d)} ,
\end{equation}
and we finish the estimate as above. Finally, if $1= \frac{2}{\tilde{p}} +\frac{2}{\tilde{q}}$ and $\tilde{q} \geq \tilde{p}$, then by \eqref{lemmaIKe1} and the fact that $(1-P_{N,e_l})$ is a bounded operator on $L^2_x (\R^d )$ we have 
\begin{multline*}
\left\|\int_J e^{-i(t-s) P} P_N h(s) ds \right\|_{L_{e_l}^{\tilde p, \tilde q} (I\times \R^d)} \\
\begin{aligned}
&=  \Bigg\| \sum_{k = 1}^d P_{N, e_k} \prod_{j = 1}^{k-1} (I - P_{N, e_{j}}) \int_J e^{-i(t -s) P } P_N h(s) ds \Bigg\|_{L_{e_l}^{\tilde p, \tilde q} (I\times \R^d)} \\
&\lesssim N^{\alpha ( \tilde{p},\tilde q)} \sum_{k = 1}^d  \bigg\|  \prod_{j = 1}^{k-1} (I - P_{N, e_{j}}) \int_J e^{is P } P_N h(s) ds \bigg \|_{L^2 (\R^d)} \\
&\lesssim N^{\alpha ( \tilde{p},\tilde q)} \sum_{k = 1}^d  \left\| \int_J e^{is P } P_N h(s) ds \right \|_{L^2 (\R^d)}
\end{aligned}
\end{multline*}
and we conclude as above. 
\end{proof}






\begin{lem}
\label{lemlin}
 Let $I\subset \R$ be a time interval such that $0=\inf I$ and fix $N\in 2^\Z$. Then, for any admissible $(p,q)$, see \eqref{defadmissible}, and any $(A, B)$ with $1= \frac{2}{B} +\frac{2}{A}$, we have
$$\left\|\int_0^t e^{-i(t-s) P} P_N h(s) ds\right\|_{L_t^q L_x^r (I\times \R^d )} \lesssim  \sum_{l=1}^d N^{\alpha (A,B)} \left\|P_N h\right\|_{L_{e_l}^{A^\prime ,B^\prime} (I\times \R^d )} \,,$$
where $\alpha(p, q)$ is as in Lemma \ref{lemmaIK}.
Moreover, if $(\tilde{p},\tilde{q}) $ is such that $1= \frac{2}{\tilde{p}} +\frac{2}{\tilde{q}}$, with $\tilde{p} \leq \tilde{q}$, then
$$ \sum_{l=1}^d\Big\|\int_0^t e^{-i(t-s) P} P_N h(s) ds \Big\|_{L_{e_l}^{\tilde{p},\tilde{q}} (I\times \R^d)}\lesssim 
N^{\alpha(\tilde{p},\tilde{q}) + \alpha (A,B)} \sum_{l=1}^d  \|P_N h\|_{L_{e_l}^{A^\prime ,B^\prime} (I\times \R^d )}. $$
\end{lem}
\begin{rmq}

\textnormal{
The main difference between 
 Lemma \ref{lemlin} and Lemma \ref{lemlinsecond} is the fact that the upper bound in the time integral in Lemma \ref{lemlin}   depends on $t$.
}
\end{rmq}


\begin{proof}
We only prove the second inequality  since the first one follows similarly.
For simplicity, we write $(p, q)$ instead of $(\tilde{p}, \tilde{q})$. 
For all $n\in \N$, there exists a partition $\{I_j^n\}_{j=1,\ldots ,2^n}$ of the interval $I$ into intervals
with disjoint interiors such that $I_j^n = I_{2j - 1}^{n + 1} \cup I_{2j}^{n+1}$ and for any small $\varepsilon > 0$
\begin{equation}
\label{lemline1}
\sum_{l=1}^d \|P_N h\|_{L_{e_l}^{p,q} (I_j^n \times \R^d)} \lesssim 2^{-(\frac{1}{2} + \varepsilon) n}\sum_{l=1}^d \|P_N h\|_{L_{e_l}^{p,q} (I \times \R^d) }.
\end{equation}
Note that $(I^n_j)_{j, n}$ is a dyadic-like partition of the interval. 

Fix $t \in I$. Then, for almost every $t_1 \in [0, t]$, there exist a unique $n\in \N$ and $j\in \{1,\ldots ,2^n\}$ such that $t_1\in I_j^n$ and $t \in I_{j+1}^n$.
Then, for $F := e^{i(t-s)P}P_N h$ and almost every $t$, we have
$$\int_0^t F(s) ds = \sum_{n\in \N} \sum_{j=1}^{2^n} \mathbbm{1}_{I^n_{j+1}}(t) \int_{I_j^n} F(s) ds, $$
where $\mathbbm{1}_{A}$ is a characteristic function of a set $A$. Consequently, since $(\sum f_j)^q = \sum f_j^q$ if 
functions $f_j$ have disjoint supports,  the triangle inequality and  $p \leq q$ (implying that $z \mapsto z^{\frac{p}{q}}$ is sub-additive) yield that  
\begin{multline*}
\bigg \|\int_0^t F(s) ds \bigg\|_{L_{e_l}^{p,q}(I \times \R^d)} \leq \sum_{n\in \N} \bigg \| \sum_{j=1}^{2^n} \mathbbm{1}_{I^n_{j+1}}(t) \int_{I^n_j} F(s) ds \bigg\|_{L_{e_l}^{p,q}(I \times \R^d)} \\
\begin{aligned}
&= \sum_{n\in \N}\bigg(\int_{\R} \bigg(\sum_{j=1}^{2^n} \int_{I_{j+1}^n} \int_{\R^{d-1}} \left|\int_{I^n_j } F(s) ds \right|^q dx^\prime dt\bigg)^{\frac{p}{q}}  dx_l \bigg)^{\frac{1}{p}}\\
&\lesssim \sum_{n\in \N} \bigg(\sum_{j=1}^{2^n} \bigg(\bigg\|\int_{I_j^n} F(s) ds \bigg\|_{L_{e_l}^{p,q} (I_{j+1}^n \times \R^d)} \bigg)^p \bigg)^{\frac{1}{p}}.
\end{aligned}
\end{multline*}
We use Lemma \ref{lemlinsecond}, \eqref{lemline1}, and $p \geq 2$  to further estimate 
\begin{multline*}
\left\|\int_0^t e^{-i(t-s) P} P_N h(s) ds \right\|_{L_{e_l}^{p,q}(I \times \R^d)}\\
\begin{aligned}
&\lesssim  \sum_{l = 1}^d \sum_{n\in \N} 2^{\frac{n}{p}} 2^{-(\frac{1}{2} + \varepsilon ) n}  N^{\alpha (p,q)} N^{\beta (A,B)} \|P_N h\|_{L_{e_l}^{A^\prime ,B^\prime}(I\times \R^d )}  \\
&\lesssim  \sum_{l = 1}^d N^{\alpha (p ,q)}  N^{\beta (A,B)} \|P_N h\|_{L_{e_l}^{A^\prime ,B^\prime} (I\times \R^d )}
\end{aligned} 
\end{multline*}
as desired.
\end{proof}





We are now in position to prove the main result of this section.

\begin{prop}
\label{mainproplinearest}
Let $I\subset \R$ be a  time interval with $t_0 \in I$ and fix admissible $(q_i, r_i)$, $i = 1, 2, 3$ and $(\tilde{p}, \tilde{q})$ with $1 = \frac{2}{\tilde{p}} + \frac{2}{\tilde{q}}$, and any $\alpha > 0$
to define the norm of $X_N(I)$. Also, fix $(A, B)$ with $1 = \frac{2}{A} + \frac{2}{B}$ to define the norm of $G_N(I)$.
For $v_0 \in \dot{H}^\alpha (\R^d)$, assume that $v:I \times \R^d \rightarrow \mathbb{C}$ is a solution to
$$\begin{cases}(i\partial_t - P)v =h \ on\ I\times \R^d , \\ v(t_0)=v_0 .  \end{cases}$$
Then, for all admissible $(q,r)$ we have
$$N^\alpha \| P_N v\|_{L_t^q L_x^r (I\times \R^d)}+ \| P_N v\|_{X_N (I)} \lesssim N^\alpha \|P_N v_0 \|_{L_x^2 (\R^d )} + \|P_N h\|_{G_N (I)}\,.$$
In particular,
$$ \| v \|_{L_t^q W_x^{\alpha ,r} (I\times \R^d)}+\|v\|_{X(I)}\lesssim \|v_0\|_{\dot{H}^\alpha (\R^d)} +\|h\|_{G(I)}.$$
\end{prop}


\begin{proof}
We assume without loss of generality that $t_0 = 0=\inf I$ and all spacial norms are on $\R^d$ and space-time norms are on $I \times \R^d$. By Duhamel's formula,
$$P_N v(t)= e^{-it P}P_N v_0 - i \int_0^t e^{-i(t-s) P} P_N h(s) ds. $$
From Lemma \ref{strichartz} and \ref{lemmaIK}, and definition of $X_N(I)$, it follows
$$N^\alpha \|e^{-it P} P_N v_0 \|_{L_t^q L_x^r } + \|e^{-it P} P_N v_0 \|_{X_N (I)} \lesssim N^\alpha \|P_N v_0\|_{L_x^2}.$$
Next, let $(q, r)$ be  admissible and let $1= \frac{2}{\tilde{p}} +\frac{2}{\tilde{q}}$. 


By Lemma \ref{strichartz} and \eqref{line1}, we find that 
\begin{align*}
N^\alpha  \left\|\int_0^t e^{-i(t-s) P} P_N h(s) ds\right\|_{L_t^q L_x^r } 
 &\leq N^\alpha  \left\|\int_I \|e^{- i t P} P_N h(s)\|_{L_x^r} ds\right\|_{L_t^q} 
 \\
 &\leq N^\alpha  
\int_I \|e^{- i t P} P_N h(s)\|_{L_t^qL_x^r} ds
\\
& \lesssim N^\alpha \int_I \|P_N h(s)\|_{L^2_x} ds \\
&= \|P_N h\|_{L^1_tL^2_x}.
\end{align*}
Analogously, by using \eqref{lemmaIKe1}, we have
\begin{multline*}
\sum_{l=1}^d N^{\alpha -\alpha (\tilde{p},\tilde q)} \left\|\int_0^t e^{-i(t-s) P} P_N h(s) ds\right\|_{L_{e_l}^{\tilde{p},\tilde q}  } \\
\begin{aligned}
 & \lesssim 
 \sum_{l=1}^d N^{\alpha -\alpha (\tilde{p},\tilde q)} \int_I \left\| e^{-it P} P_N h(s) \right\|_{L_{e_l}^{\tilde{p},\tilde q}  } ds \\
 &\lesssim \|P_N h\|_{L^1_tL^2_x}.
\end{aligned}
\end{multline*}
Using Lemma \ref{lemlin}, we also obtain for $1= \frac{2}{B} +\frac{2}{A}$ that
\begin{multline*}
N^\alpha  \left\|\int_0^t e^{-i(t-s) P} P_N h(s) ds\right\|_{L_t^q L_x^r} + \sum_{l=1}^d N^{\alpha -\alpha (\tilde{p},\tilde q )}   
\left\|\int_0^t e^{-i(t-s) P} P_N h(s) ds\right\|_{L_{e_l}^{\tilde{p},\tilde q}  }\\
 \lesssim N^{\alpha +\beta (A,B)}  \|P_N h\|_{L_{e_l}^{A^\prime ,B^\prime} } .
\end{multline*}
Since $(q, r)$ was an arbitrary admissible pair, we can repeat the same reasoning with $(q, r) = (q_i, r_i)$, $i = 1, 2, 3$ and obtain the desired result.
\end{proof}

We also need to define another norm $Y(I)$ which is used to control parts of the nonlinearity. As for the norm on $X(I)$ and $G(I)$, the definition relies on a dyadic decomposition
$$\|f\|_{Y(I)} = \Big(\sum_{N\in 2^{\Z}}  \|P_N f\|_{Y_N(I)}^2\Big)^{\frac{1}{2}},$$
with
\begin{equation}\label{defyn}
\begin{aligned}
\|P_N f\|_{Y_N(I)}&= \langle N\rangle^C \left(\|P_N f\|_{L_t^{\frac{2(s+d)}{d}} L_x^{\frac{2(s+d)}{s}} (I\times \R^d)} +\|P_N f\|_{L_t^{\frac{2(s+d)}{s}} L_x^{\frac{2(s+d)}{s}}(I\times \R^d) } \right)\\
&+\langle N\rangle^A \sum_{l=1}^d \|P_{N,e_l} P_N f \|_{L^{\frac{4}{\varepsilon},\frac{4}{2-\varepsilon}}_{e_l}(I\times \R^d) } +N^{-B} \sum_{l=1}^d \|P_N f\|_{L^{\frac{4}{2-\varepsilon},\frac{4}{\varepsilon}}_{e_l} (I\times \R^d) },  
\end{aligned}
\end{equation}
for some positive constants $A,B$, and $C$ fixed below.



Finally, we conclude this section by stating a continuity property for the norms $X(I)$ and $Y(I)$ with respect to the interval which follows from the dominated convergence theorem.

\begin{lem}
\label{lemcontt}
 Let $I\subset \R$ be a closed interval. Assume that $\|v\|_{X(I)}<\infty $  and $\|F\|_{Y(I)}<\infty$. Then the mappings
$$t\in I \rightarrow \|v\|_{X([\inf I,t])},\qquad t\in I \rightarrow \|F\|_{Y([\inf I,t])}$$
and
$$t\in I \rightarrow \|v\|_{X([t,\sup I])},\qquad t\in I \rightarrow \|F\|_{Y([t,\sup I])}$$
are continuous. We can also allow half-open and open intervals $I$.
\end{lem}

\section{Nonlinear estimates}\label{sec:nonlin}
Our goal in this section is to estimate the $G (I)$ norm of $|F+v|^2 (F+v)$, where $v$ is a fixed function 
with finite $X(I)$ norm and $F$ is a lower regularity forcing term with finite $Y(I)$ norm. By  the definition of $G(I)$, it suffices to
bound either norm of $L^1_t L^2_x$ or $L_{e_l}^{A^\prime,B^\prime}$. 
First, if $v$ appears at the 
highest frequency, we  use the $L^1_t L^2_x$ norm and the proof uses a combination of H\" older and Strichartz estimates. Second, if $F$ appears at the highest frequency, 
we  use the $L_{e_l}^{A^\prime,B^\prime}$ norm for $A^\prime\approx 1+$ and $B^\prime\approx 2-$. Then, 
$\beta (A,B)=\beta (\infty- , 2+)= \frac{1-s}{2}-$ which allows us to  gain some derivatives. Also, we bound the highest frequency term $F$ in the $L^{\infty- ,2+}$ component of the $Y_N$ norm 
gaining once more $ \frac{1-s}{2}-$ derivatives with the help of \eqref{IK3}. We control the remaining terms by using Strichartz and maximal function type $L_{e_l}^{2+,\infty-}$ estimates. 
In the following proposition, we set 
\begin{equation}\label{parms}
\begin{aligned}
\alpha = \frac{d - s}{2} &\qquad \tilde{p} = \frac{4}{2 - \varepsilon} \\
(q_1, q_2, q_3) &= \left(2, \frac{2(s+d)}{d}, \frac{2(s+d)}{s}\right), \\
 (r_1, r_2, r_3) &= \left(\frac{2d}{d-s}, \frac{2(s+d)}{d}, \frac{2(s+d)d}{(s+d)d - s^2}\right) \,.
\end{aligned}
\end{equation}
in the definition of  $X_N$ (see \eqref{dfxn})
and recall the definition of $Y_N$: 
\begin{align*}
\|P_N f\|_{Y_N(I)}&= \langle N\rangle^C \left(\|P_N f\|_{L_t^{\frac{2(s+d)}{d}} L_x^{\frac{2(s+d)}{s}} (I\times \R^d)} +\|P_N f\|_{L_t^{\frac{2(s+d)}{s}} L_x^{\frac{2(s+d)}{s}}(I\times \R^d) } \right)\\
&+\langle N\rangle^A \sum_{l=1}^d \|P_{N,e_l} P_N f \|_{L^{\frac{4}{\varepsilon},\frac{4}{2-\varepsilon}}_{e_l}(I\times \R^d) } +N^{-B} \sum_{l=1}^d \|P_N f\|_{L^{\frac{4}{2-\varepsilon},\frac{4}{\varepsilon}}_{e_l} (I\times \R^d) },  
\end{align*}
where $A, B, C \geq 0$, and $\varepsilon \in (0, 2)$ are chosen such that the following holds true
\begin{equation}\label{recc}
C \geq \frac{s(d-s)}{4(s+d)},
\end{equation} 
\begin{gather}
\label{relAB}
A\geq \frac{d+1}{2} -s +\varepsilon \dfrac{d+s-2}{4} +2B  \\
\label{relAB1}
A\theta_1 +C (1-\theta_1)>\beta \left( \frac{4}{\varepsilon}, \frac{4}{2-\varepsilon}\right)+\frac{d-s}{2},\qquad \theta_1=\frac{d-s - \alpha (\frac{4}{2-\varepsilon},\frac{4}{\varepsilon})}{\alpha (\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon})}, \\
\label{relAB2}
A\theta_2 +C(1-\theta_2 )> \beta \left(\frac{4}{\varepsilon}, \frac{4}{2-\varepsilon}\right) +\frac{d-s}{2} +B,\qquad \theta_2= \frac{d-s}{2\alpha  (\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}) }. 
\end{gather}
Notice that \eqref{relAB} and \eqref{relAB1} after observing that $\theta_2 = (\theta_1 + 1)/2$, imply  \eqref{relAB2}, thus it can be neglected. 
Below, we check that such choice of $A, B, C \geq 0$, and $\varepsilon \in (0, 2)$ is indeed possible.

\begin{prop}
\label{mainpropnonlinear}
Let $N \lesssim N_1 $ and $N_1 \geq N_2 \geq N_3$ be dyadic integers $2^{\Z}$. Let $I \subset \R$ be an interval. Assume that $\varepsilon < 2\frac{s-1}{d+s-2}$ and $\varepsilon>2 \frac{2s-d-1}{d+s-2}$ if $s>\frac{d+1}{2}$. Then, for any $e\in \{e_1, \cdots , e_d\}$, and already fixed constants in $X_N(I)$ and $Y_N(I)$ norms we have the following estimates:

\noindent
Term (1):
\begin{multline*}
N^\alpha \|P_N (P_{N_1} v_1 P_{N_2} v_2 P_{N_3}v_3)\|_{L^1_t L_x^2}\\
\lesssim \left(\dfrac{N}{N_1}\right)^{\alpha} \left(\dfrac{N_3}{N_2}\right)^{\alpha_1} \|P_{N_1}v_1\|_{X_{N_1}}\|P_{N_2}v_2\|_{X_{N_2}} \|P_{N_3}v_3\|_{X_{N_3}}. 
\end{multline*}
\noindent
Term (2):
\begin{multline*}
N^\alpha \|P_N (P_{N_1} v_1 P_{N_2} F_2 P_{N_3}v_3)\|_{L^1_t L_x^2}\\
\lesssim \left(\dfrac{N}{N_1}\right)^{\alpha} \left(\frac{N_3}{N_2} \right)^{\frac{(d-s)s}{4(d+s)}}  \|P_{N_1}v_1\|_{X_{N_1}}\|P_{N_2}F_2\|_{Y_{N_2}} \|P_{N_3}v_3\|_{X_{N_3}}. 
\end{multline*}
\noindent
Term (3):
\begin{multline*}
N^\alpha \|P_N (P_{N_1} v_1 P_{N_2} F_2 P_{N_3}F_3)\|_{L^1_t L_x^2}\\
\lesssim \left(\dfrac{N}{N_1}\right)^{\alpha} \left(\dfrac{N_3}{N_2}\right)^{C} \|P_{N_1}v_1\|_{X_{N_1}}\|P_{N_2}F_2\|_{Y_{N_2}} \|P_{N_3}F_3\|_{Y_{N_3}}. 
\end{multline*}
\noindent
Term (4):
\begin{multline*}
N^\alpha \|P_N (P_{N_1} v_1 P_{N_2} v_2 P_{N_3}F_3)\|_{L^1_t L_x^2}\\
\lesssim \left(\dfrac{N}{N_1}\right)^{\alpha} \left(\dfrac{N_3}{N_2}\right)^{\frac{ds}{2(s+d)}} \|P_{N_1}v_1\|_{X_{N_1}}\|P_{N_2}v_2\|_{X_{N_2}} \|P_{N_3}F_3\|_{Y_{N_3}}. 
\end{multline*}
\noindent
Term (5):
\begin{multline*}
N^{\beta (\frac{4}{\varepsilon},\frac{4}{2-\varepsilon}) +\alpha} \|P_N (P_{N_1} F_1 P_{N_2} F_2 P_{N_3} F_3) \|_{L_e^{\frac{4}{4-\varepsilon} , \frac{4}{2+\varepsilon}} }\\
 \lesssim \left(\dfrac{N}{N_1}\right)^{\beta  (\frac{4}{\varepsilon},\frac{4}{2-\varepsilon})+ \alpha} \left(\dfrac{N_2}{N_1}\right)^B \left(\dfrac{N_3}{N_1}\right)^{B} 
 \|P_{N_1}F_1\|_{Y_{N_1}}\|P_{N_2}F_2\|_{Y_{N_2}} \|P_{N_3}F_3\|_{Y_{N_3}}.  
\end{multline*}
\noindent
Term (6):
\begin{multline*}
N^{\beta (\frac{4}{\varepsilon},\frac{4}{2-\varepsilon}) +\alpha} \|P_N (P_{N_1} F_1 P_{N_2} v_2 P_{N_3} v_3) \|_{L_e^{\frac{4}{4-\varepsilon} , \frac{4}{2+\varepsilon}} }\\
 \lesssim \left(\dfrac{N}{N_1}\right)^{\beta (\frac{4}{\varepsilon},\frac{4}{2-\varepsilon})+\alpha} \left(\dfrac{N_3}{N_2}\right)^{\alpha  (\frac{4}{2-\varepsilon},\frac{4}{\varepsilon}) -\alpha} \|P_{N_1}F_1\|_{Y_{N_1}}\|P_{N_2}v_2\|_{X_{N_2}} \|P_{N_3}v_3\|_{X_{N_3}}.  
\end{multline*}
\noindent
Term (7):
\begin{multline*}
N^{\beta (\frac{4}{\varepsilon},\frac{4}{2-\varepsilon}) +\alpha} \|P_N (P_{N_1} F_1 P_{N_2} F_2 P_{N_3} v_3) \|_{L_e^{\frac{4}{4-\varepsilon} , \frac{4}{2+\varepsilon}} }\\
 \lesssim \left(\dfrac{N}{N_1}\right)^{\beta  (\frac{4}{\varepsilon},\frac{4}{2-\varepsilon})+\alpha} \left(\dfrac{N_2}{N_1}\right)^{B} \|P_{N_1}F_1\|_{Y_{N_1}}\|P_{N_2}F_2\|_{Y_{N_2}} \|P_{N_3}v_3\|_{X_{N_3}}.  
\end{multline*}
\noindent
Term (8):
\begin{multline*}
N^{\beta (\frac{4}{\varepsilon},\frac{4}{2-\varepsilon}) +\alpha} \|P_N (P_{N_1} F_1 P_{N_2} v_2 P_{N_3} F_3) \|_{L_e^{\frac{4}{4-\varepsilon} , \frac{4}{2+\varepsilon}} }\\
 \lesssim \left(\dfrac{N}{N_1}\right)^{\beta  (\frac{4}{\varepsilon},\frac{4}{2-\varepsilon})+\alpha} \left(\dfrac{N_3}{N_1}\right)^{B} \|P_{N_1}F_1\|_{Y_{N_1}}\|P_{N_2}v_2\|_{X_{N_2}} \|P_{N_3}F_3\|_{Y_{N_3}}.  
\end{multline*}
Here, all the norms are taken over $I\times \R^d$, $\alpha = \frac{d-s}{2}$ is as in the definition of $X_N$, and $\alpha_1>0$
is a number determined by other parameters, but independent of $N$. 
\end{prop}

\begin{cor}\label{cor:wis}
Assume the assumptions of Proposition \ref{mainpropnonlinear}, and set $(A', B') = (\frac{4}{4 - \varepsilon}, \frac{4}{2 + \varepsilon})$, that is, $(A, B) = (\frac{4}{\varepsilon}, \frac{4}{2 - \varepsilon})$ 
in the definition of $G(I)$. Then, 
for any $F\in Y(I)$, and any $v,v_1,v_2\in X(I)$, we have
$$\||F+v|^2 (F+v)\|_{G(I)}\lesssim \|F\|_{Y(I)}^3 +\|v\|_{X(I)}^3,$$
and
\begin{multline*}
\||F+v_1|^2 (F+v_1) - |F+v_2|^2 (F+v_2) \|_{G(I)} \\
\lesssim \|v_1 - v_2\|_{X(I)} (\|F\|^2_{Y(I)} +\|v_1\|_{X(I)}^2 +\|v_2\|_{X(I)}^2).
\end{multline*}
\end{cor}

\begin{proof}
After standard algebraic manipulations and a use of triangle inequality, we have to estimate terms of the form $\|g_1 g_2g_3\|_{G(I)}$, where $g_i$ stands for either $F$, $v$, or their conjugates
in the first claim and for $F$, $v_i$, $v_1 - v_2$, or their conjugates in the second claim. By the definition of $G(I)$, for each $N \in 2^\Z$ it suffices  to estimate $P_N(g_1 g_2g_3)$, which equals
$$
P_N(g_1 g_2g_3) = \sum_{\sigma} \sum_{N_1, N_2, N_3 \in 2^\Z} P_N(P_{N_1}g_{\sigma(1)} P_{N_2}g_{\sigma(2)} P_{N_3} g_{\sigma(3)})  \,,
$$
where the first sum is over all permutations $\sigma$ of the set $\{1, 2, 3\}$.
After relabeling, we can without loss of generality assume $N_1 \geq N_2 \geq N_3$. Since $P_{N_i} g_i$ is supported in the Fourier space around $\approx N_i$ and Fourier transform 
maps products to convolutions, we obtain that $P_N(P_{N_1}g_1 P_{N_2}g_2 P_{N_3} g_3) \equiv 0$ if $N \gtrsim N_1+ N_2 + N_3$, and in particular for $N \gtrsim N_1$. 
Hence, by the triangle inequality, it suffices to estimate  $P_N(P_{N_1}g_1 P_{N_2}g_2 P_{N_3} g_3)$ in either in $L^1_tL^2_x$ or $L^{A', B'}_{e_l}$ norm for any $N_3 \leq N_2 \leq N_1$ and $N \lesssim N_1$, 
where the norm on $g_i$ is either $X(I)$ or $Y(I)$ and $g_i$ is respectively $v_j$ (or the difference of some $v$) or $F$.

For an illustration, we estimate the term $P_N(P_{N_1} v P_{N_2} F P_{N_3} F)$, in $L^1_tL^2_x$, all other terms follow similarly. By Proposition \ref{mainpropnonlinear}, term (3) 
and $N_3 \leq N_2$ we have
\begin{multline*}
N^\alpha \|P_N (P_{N_1} v P_{N_2} F P_{N_3}F)\|_{L^1_t L_x^2} \\
\lesssim \left(\dfrac{N}{N_1}\right)^{\zeta}   \left(\dfrac{N}{N_1}\right)^{\eta}  \|P_{N_1}v\|_{X_{N_1}}\|P_{N_2}F\|_{Y_{N_2}} \|P_{N_3}F\|_{Y_{N_3}}
\end{multline*} 
for some $\zeta, \eta > 0$. 
Then,  the definition of $G_N$ yields
\begin{multline*}
\|P_N(vF^2)\|_{G_N(I)}  \\
\lesssim  \sum_{\substack{N_1, N_2, N_3 \in 2^\Z\\ N_1 \geq N_2 \geq N_3}}  \left(\dfrac{N}{N_1}\right)^{\zeta} \left(\dfrac{N_3}{N_2}\right)^{\eta} \|P_{N_1}v\|_{X_{N_1}}\|P_{N_2}F\|_{Y_{N_2}} \|P_{N_3}F\|_{Y_{N_3}}\,,
\end{multline*}
and consequently by dropping the restriction $N_2 \leq N_1$
\begin{multline*}
\|vF^2\|_{G(I)}^2 \\
\begin{aligned}
 &\lesssim \sum_{N \in 2^\Z}  \Bigg(\sum_{\substack{N_1, N_2, N_3 \in 2^\Z\\ N_1 \geq N_2 \geq N_3\\ N \lesssim N_1}} 
 \left(\dfrac{N}{N_1}\right)^{\zeta}  \left(\dfrac{N_3}{N_2}\right)^{\eta}  \|P_{N_1}v\|_{X_{N_1}}\|P_{N_2}F\|_{Y_{N_2}} \|P_{N_3}F\|_{Y_{N_3}} \Bigg)^2 \\
 &\lesssim \sum_{N \in 2^\Z}  \Bigg(\sum_{\substack{N_1 \in 2^\Z \\N \lesssim N_1}} \left(\dfrac{N}{N_1}\right)^{\zeta}  \|P_{N_1}v\|_{X_{N_1}}\Bigg)^2 \\
 & \qquad  \times
\Bigg( \sum_{\substack{N_2, N_3 \in 2^\Z \\ N_2 \geq  N_3}} \left(\dfrac{N_3}{N_2}\right)^{\eta}  \|P_{N_2}F\|_{Y_{N_2}} \|P_{N_3}F\|_{Y_{N_3}} \Bigg)^2 \,.
\end{aligned}
\end{multline*}
Next, using discrete convolution inequality and summability of $\left(\frac{N}{N_1}\right)^{\zeta}$ in $N \lesssim N_1$, we obtain
\begin{align*}
 \sum_{N \in 2^\Z}  \Bigg(\sum_{\substack{N_1 \in 2^\Z \\ N \lesssim N_1}} \left(\dfrac{N}{N_1}\right)^{\zeta}  \|P_{N_1}v\|_{X_{N_1}}\Bigg)^2  \lesssim  \sum_{N_1 \in 2^\Z} \|P_{N_1}v\|_{X_{N_1}}^2 = \|v\|_{X}^2 \,.
\end{align*}
Also, using Cauchy inequality and discrete convolution inequality, we have 
\begin{multline*}
\Bigg( \sum_{\substack{N_2, N_3 \in 2^\Z \\ N_2 \geq  N_3}} \left(\dfrac{N_3}{N_2}\right)^{\eta}  \|P_{N_2}F\|_{Y_{N_2}} \|P_{N_3}F\|_{Y_{N_3}} \Bigg)^2  \\ 
\begin{aligned}
&=\Bigg(\sum_{N_2\in 2^\Z} \|P_{N_2}F\|_{Y_{N_2}}  \sum_{\substack{ N_3 \in 2^\Z \\ N_2 \geq  N_3}} \left(\dfrac{N_3}{N_2}\right)^{\eta}  \|P_{N_3}F\|_{Y_{N_3}} \Bigg)^2 \\
&\leq \sum_{N_2\in 2^\Z} \|P_{N_2}F\|_{Y_{N_2}}^2 \sum_{N_2\in 2^\Z} \Bigg(\sum_{\substack{N_3 \in 2^\Z \\ N_2 \geq  N_3}} \left(\dfrac{N_3}{N_2}\right)^{\eta}  \|P_{N_3}F\|_{Y_{N_3}} \Bigg)^2 \\
&\lesssim \|F\|_Y^2 \sum_{N_3\in 2^\Z}\|P_{N_3}F\|_{Y_{N_3}} ^2 \\
&= \|F\|_Y^4 ,
\end{aligned}
\end{multline*}
as desired. 
\end{proof}


\begin{proof}[Proof of Proposition \ref{mainpropnonlinear}]
We begin by proving the first four estimates. We use repeatly H\" older's inequality and Bernstein estimates (see Lemma \ref{bern}). To simplify our notation, for fixed $\varepsilon$, we
 denote 
$$
\beta := \alpha \left( \frac{4}{\varepsilon}, \frac{4}{2-\varepsilon}\right) =  \beta \left(\frac{4}{\varepsilon}, \frac{4}{2-\varepsilon}\right) = \dfrac{1-s}{2}+\varepsilon \dfrac{d+s-2}{4} \,.
$$
Let us recall that
$$\|P_N v\|_{X_N (I)}= N^\alpha \sum_{i=1}^{3} \|P_N v\|_{L_t^{q_i} L_x^{r_i} (I\times \R^d)} +N^{\alpha-\alpha(\tilde{p},\tilde{q}) } \sum_{l=1}^d \|P_N v \|_{L_{e_l}^{\tilde{p},\tilde{q} } (I\times \R^d)},$$
where $(q_i ,r_i)$, $i=1,2,3$, are admissible couples, $1= \frac{2}{\tilde{p}} +\frac{2}{\tilde{q}}$ and $\alpha>0$ fixed in \eqref{parms}. Note that the choice of $q_i$ implies 
\begin{equation}\label{soq}
\sum_{i=1}^3 \dfrac{1}{q_i}=1. 
\end{equation}



\textbf{Term (1)}: $v_1 v_2 v_3$. 
Fix $p_i$, $i = 1, 2, 3$ such that $\frac{1}{2}=\frac{1}{p_1}+\frac{1}{p_2} +\frac{1}{p_3} $, with $p_1=r_1$, $p_2 \geq r_2$ and $p_3 > \max\{p_3, \frac{2(d + s)}{s} \}$. 
Then, since $P_N$ is bounded on $L^2$, H\" older
inequality,  Lemma \ref{bern} with $(r_1, r_2) = (r_i, p_i)$ and the definition of $X_N$ imply
\begin{align*}
N^\alpha \|P_N (P_{N_1} v_1  P_{N_2} v_2 P_{N_3} v_3) \|_{L_t^{1} L_x^{2}} &\lesssim N^\alpha \prod_{i=1}^3 \|P_{N_i} v_i \|_{L_t^{q_i}L_x^{p_i}} \\
&\lesssim N^\alpha \prod_{i=1}^3 N_i^{\frac{d}{r_i}- \frac{d}{p_i}} \|P_{N_i} v_i \|_{L_t^{q_i}L_x^{r_i}} \\
&\lesssim N^\alpha \prod_{i=1}^3 N_i^{\frac{d}{r_i}- \frac{d}{p_i} -\alpha } \|P_{N_i} v_i\|_{X_{N_i}}  .
\end{align*}
Using \eqref{soq}, $\frac{1}{2} = \frac{1}{r_1} + \frac{1}{r_2} + \frac{1}{r_3}$, $(q_1,p_1) = (q_1, r_1)$ is admissible, and $\alpha =\frac{d-s}{2}$, we have
$$\alpha_1 :=  \frac{d}{ r_3} - \frac{d}{p_3} -\alpha = -\left(\frac{d}{ r_2}- \frac{d}{p_2} -\alpha\right) > 0 \,,$$
where the last inequality holds for any $p_3 > \frac{2(d + s)}{s}$, as  assumed. 
Hence,
$$N^\alpha \|P_N (P_{N_1} v_1  P_{N_2} v_2 P_{N_3} v_3) \|_{L_t^{1} L_x^{2}} \lesssim \left(\frac{N}{N_1}\right)^\alpha \left(\frac{N_3}{N_2}\right)^{\alpha_1} \prod_{i=1}^3 \|P_{N_i} v_i \|_{X_{N_i}} \,,$$
as desired.


\textbf{Term (2)}: $v_1 F_2 v_3$. By the definition of $(q_i, r_i)$, H\" older inequality, 
and Lemma \ref{bern} with $(r_1, r_2) = (r_2, \frac{2d (s+d)}{s^2})$, we have
\begin{multline*}
N^\alpha \|P_N (P_{N_1} v_1  P_{N_2} F_2 P_{N_3} v_3) \|_{L_t^{1} L_x^{2}} \\
\begin{aligned}
& \lesssim  N^\alpha \|P_{N_1} v_1\|_{L_t^{q_1}L_x^{r_1}} \|P_{N_2} F_2\|_{L_t^{q_3 } L_x^{q_3}} \|P_{N_3} v_3\|_{L_t^{q_2} L_x^{\frac{2d (s+d)}{s^2}}}\\
& \lesssim \left(\frac{N}{N_1}\right)^\alpha \|P_{N_1} v_1\|_{X_{N_1}} \langle N_2 \rangle^{-C} \|P_{N_2} F_2\|_{Y_{N_2}}  N_3^{\frac{d-s}{2} } \|P_{N_3} v_3\|_{L_t^{q_2} L_x^{r_2}}  \\
&\lesssim  \left(\frac{N}{N_1}\right)^\alpha \langle N_2 \rangle^{-C}  \|P_{N_1} v_1\|_{X_{N_1}} \|P_{N_2} F_2\|_{Y_{N_2}} \|P_{N_3} v_3\|_{X_{N_3}} \,.
\end{aligned}
\end{multline*}
Similarly, by H\" older inequality, 
and Lemma \ref{bern} with $(r_1, r_2) = (r_3, \frac{2d (s+d)}{s^2})$, we have
\begin{multline*}
N^\alpha \|P_N (P_{N_1} v_1  P_{N_2} F_2 P_{N_3} v_3) \|_{L_t^{1} L_x^{2}} \\
\begin{aligned}
& \lesssim  N^\alpha \|P_{N_1} v_1\|_{L_t^{q_1}L_x^{r_1}} \|P_{N_2} F_2\|_{L_t^{q_2 } L_x^{q_3}} \|P_{N_3} v_3\|_{L_t^{q_3} L_x^{\frac{2d (s+d)}{s^2}}}\\
&\lesssim  \left(\frac{N}{N_1}\right)^\alpha  \|P_{N_1} v_1\|_{X_{N_1}} \langle N_2 \rangle^{-C}  \|P_{N_2} F_2\|_{Y_{N_2}} N_3^{\frac{(d-s)s}{2(d+s)}} \|P_{N_3} v_3\|_{X_{N_3}} \,.
\end{aligned}
\end{multline*}
Using the geometric mean of the obtained estimates, 
\begin{multline*}
N^\alpha \|P_N (P_{N_1} v_1  P_{N_2} F_2 P_{N_3} v_3) \|_{L_t^{1} L_x^{2}} \\
\begin{aligned}
&\lesssim  \left(\frac{N}{N_1}\right)^\alpha  \|P_{N_1} v_1\|_{X_{N_1}} \langle N_2 \rangle^{-C} N_3^{\frac{(d-s)s}{4(d+s)}} \|P_{N_2} F_2\|_{Y_{N_2}}  \|P_{N_3} v_3\|_{X_{N_3}} 
\end{aligned}
\end{multline*}
and the desired estimate follows from \eqref{recc}.

\textbf{Term (3)}: $v_1 F_2 F_3$.
 By the definition of $(q_i, r_i)$, H\" older inequality,  Lemma \ref{bern} with $(r_1, r_2) = (q_3, \frac{2d(s+d)}{s^2})$, and $C > \frac{s(d-s)}{4(d + s)}$ we have
\begin{multline*}
 N^\alpha \|P_N (P_{N_1} v_1  P_{N_2} F_2 P_{N_3} F_3) \|_{L_t^{1} L_x^{2}}\\
\begin{aligned}
& \lesssim  N^\alpha \|P_{N_1} v_1\|_{L^{q_1}_tL^{r_1}_x} \|P_{N_2} F_2\|_{L_t^{q_2 } L_x^{q_3} } \|P_{N_3} F_3\|_{L_t^{q_3} L_x^{\frac{2d(s+d)}{s^2}}}\\
& \lesssim \left(\frac{N}{N_1}\right)^\alpha \|P_{N_1} v_1\|_{X_{N_1}} \langle N_2\rangle^{-C} \|P_{N_2} F_2\|_{Y_{N_2} }   N_3^{\frac{s(d-s)}{2(d + s)}} \|P_{N_3} F_3\|_{L_t^{q_3}L_x^{q_3}}  \\
&\lesssim  \left(\frac{N}{N_1}\right)^\alpha \langle N_2\rangle^{-C} \|P_{N_1} v_1\|_{X_{N_1}} N_3^{\frac{s(d-s)}{2(d + s)}} \langle N_3\rangle^{-C}\|P_{N_2} F_2\|_{Y_{N_2}} \|P_{N_3} F_3\|_{Y_{N_3}} \,.
\end{aligned}
\end{multline*}
However, by \eqref{recc}
$$
\langle N_2\rangle^{-C} N_3^{\frac{s(d-s)}{2(d + s)}} \langle N_3\rangle^{-C} \leq \langle N_2\rangle^{-C} N_3^{\frac{s(d-s)}{4(d + s)}} \leq  \left(\frac{N_3}{N_2}\right)^{\frac{s(d-s)}{4(d + s)}}
$$
and the desired bound is proved. 


\textbf{Term (4)}:  $v_1 v_2 F_3$.
 By the definition of $(q_i, r_i)$, H\" older inequality,  Lemma \ref{bern} with $(r_1, r_2) = (r_2, \frac{2d}{s})$ and $(r_1, r_2) = (r_3, \infty)$, and $1 \leq \langle N\rangle^C $ we have
\begin{multline*}
N^\alpha \|P_N (P_{N_1} v_1  P_{N_2} v_2 P_{N_3} F_3) \|_{L_t^{1} L_x^{2}}\\
\begin{aligned}
& \lesssim \left(\frac{N}{N_1}\right)^\alpha \|P_{N_1} v_1\|_{X_{N_1}} \|P_{N_2} v_2\|_{L_t^{q_2} L_x^{\frac{2d}{s} }} \|P_{N_3} F_3\|_{L_t^{q_3} L_x^{\infty}}\\
& \lesssim \left(\frac{N}{N_1}\right)^\alpha \|P_{N_1} v_1\|_{X_{N_1}}   N_2^{-\frac{sd}{2(s+d)}} \|P_{N_2} v_2\|_{X_{N_2}} N_3^{\frac{ds}{2(s+d)}}\|P_{N_3} F_3\|_{ L_t^{q_3 } L_x^{q_3} }  \\
&\lesssim  \left(\frac{N}{N_1}\right)^\alpha \left(\frac{N_3}{N_2}\right)^{\frac{ds}{2(s+d)}} \|P_{N_1} v_1\|_{X_{N_1}}  \|P_{N_2} v_2\|_{X_{N_2}} \|P_{N_3} F_3\|_{Y_{N_3}}.
\end{aligned}
\end{multline*}

Next, we focus on last four terms involving the anisotropic norm $L_{e_l}^{p,q}$. 
Note that $\beta +\alpha>0$ holds if $s<\frac{d+1}{2}$ or $s>\frac{d+1}{2}$ and $\varepsilon> 2\frac{2s-d-1}{d+s-2}$ as assumed.


\textbf{Term (5)}: $F_1 F_2 F_3$. 
Fix $l \in \{1, \cdots, d\}$ and let $e := e_l$.
Since $P_N$ is induced by the convolution with globally $L^1_x$ function, it is bounded on any $L^p$ space, and  then H\"older inequality implies
\begin{multline*}
N^{\beta +\alpha} \|P_N (P_{N_1} F_1 P_{N_2} F_2 P_{N_3} F_3) \|_{L^{\frac{4}{4-\varepsilon} ,\frac{4}{2+\varepsilon}}_e}\\
 \lesssim N^{\beta +\alpha} \|P_{N_1} F_1\|_{L_e^{\frac{4}{\varepsilon} ,\frac{4}{2-\varepsilon}}} \|P_{N_2} F_2\|_{L_e^{\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}}} \|P_{N_3} F_3\|_{L_e^{\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}}}.
\end{multline*}
Next, by \eqref{e1}
\begin{align*}
P_{N_1}&= P_{N_1} \sum_{k = 1}^d P_{N_1 ,e_k} \prod_{j = 1}^{k - 1} (I - P_{N_1, e_j})
\,.
\end{align*}
Let $\tilde{P}_N$ be a Littlewood-Paley projection with the symbol  $\varphi(8\xi/N) - \varphi(\xi/8N)$, where $\varphi$ is as in the definition of $P_N$ in \eqref{LPproj}. 
Then, $P_{N_1} = P_{N_1}\tilde{P}_N$,
and since for any $k \in \{1, \cdots, d\}$ the operator $\prod_{j = 1}^{k - 1} (I - P_{N_1, e_j}) \tilde{P}_{N_1}$ has a kernel uniformly bounded in $L_x^1$, we have
\begin{multline*}
N^{\beta +\alpha} \|P_N (P_{N_1} F_1 P_{N_2} F_2 P_{N_3} F_3) \|_{L^{\frac{4}{4-\varepsilon} ,\frac{4}{2+\varepsilon}}_e}\\
\begin{aligned}
& \lesssim N^{\beta +\alpha}\sum_{l=1}^d \|P_{N_1,e_l} P_{N_1} F_1\|_{L_e^{\frac{4}{\varepsilon} ,\frac{4}{2-\varepsilon}}} \|P_{N_2} F_2\|_{L_e^{\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}}} \|P_{N_3} F_3\|_{L_e^{\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}}}\\
&\lesssim N^{\beta +\alpha} \langle N_1\rangle^{-A}  \| P_{N_1} F_1\|_{Y_{N_1}} N_2^B \|P_{N_2} F_2\|_{Y_{N_2}} N_3^B \|P_{N_3} F_3\|_{Y_{N_3}}.
\end{aligned}
\end{multline*}
By \eqref{relAB}
$$
\alpha+\beta= \frac{d+1}{2} -s +\varepsilon \dfrac{d+s-2}{4} = A - 2B \,,
$$
and after a rearrangement, 
we obtain the desired conclusion. 


\textbf{Term (6)}: $F_1 v_2 v_3$.
By H\"older's inequality for any $k \in \{1, \cdots, d\}$
$$
\|\|G_1 G_2\|_{L^{\frac{4}{2+\varepsilon}}_{x',t}}\|_{L^{\frac{4}{4-\varepsilon}}_{x_k}} \leq \| \|G_1\|_{L^2_{x',t}} \|G_2\|_{L^{\frac{4}{\varepsilon}}_{x',t}} \|_{L^{\frac{4}{4-\varepsilon}}_{x_k}}
\leq \|G_1\|_{L^2_{x, t}} \|G_1\|_{L^{\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}}_{e_k}}
$$
and similarly for $e = e_k$
$$N^{\beta +\alpha} \|P_{N_1}F_1 P_{N_2} v_2 P_{N_3} v_3\|_{L^{\frac{4}{4-\varepsilon} ,\frac{4}{2+\varepsilon}}_e} \lesssim  
N^{\beta +\alpha} \|P_{N_1}F_1 P_{N_2} v_2\|_{L^{2,2}_e}  \|P_{N_3} v_3\|_{L^{\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}}_e}. $$
Since $L_e^{2,2}=L_t^2 L_x^2$, then H\"older's inequality  implies
$$
 \|P_{N_1}F_1 P_{N_2} v_2\|_{L_t^2 L_x^2} \leq  \|P_{N_2} v_2\|_{L_t^{q_2} L_x^{q_2}} \|P_{N_1}F_1 \|_{L_t^{q_3} L_x^{q_3}}
$$
and the same decomposition of $P_{N_1}$ as in the estimate of term (5) yields
$$
 \|P_{N_1}F_1 P_{N_2} v_2\|_{L_t^2 L_x^2} \lesssim \sum_{l=1}^d \|P_{N_1 ,e_l}P_{N_1} F_1\|_{L_{e_l}^{\frac{4}{\varepsilon} ,\frac{4}{2-\varepsilon}}}  \sum_{l=1}^d\|P_{N_2}v_2\|_{L_{e_l}^{\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}}}
$$
Thus, for $\theta_1$ as in  \eqref{relAB1} (see below that $\theta_1 \in (0, 1)$), we have 
\begin{align*}
 \|P_{N_1}F_1 P_{N_2} v_2\|_{L_t^2 L_x^2} &\lesssim 
 \left(\sum_{l=1}^d \|P_{N_1 ,e_l}P_{N_1} F_1\|_{L_{e_l}^{\frac{4}{\varepsilon} ,\frac{4}{2-\varepsilon}}}  \sum_{l=1}^d\|P_{N_2}v_2\|_{L_{e_l}^{\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}}} \right)^{\theta_1} \\
& \qquad \times \left( \|P_{N_1}F_1 \|_{L_t^{q_3} L_x^{q_3}} \|P_{N_2} v_2\|_{L_t^{q_2} L_x^{q_2}} \right)^{1-\theta_1} \,.
\end{align*}
Then, 
\begin{multline*}
N^{\beta +\alpha}  \|P_{N_1}F_1 P_{N_2} v_2 P_{N_3} v_3\|_{L^{\frac{4}{4-\varepsilon} ,\frac{4}{2+\varepsilon}}_e} \\
\begin{aligned}
&\lesssim  N^{\beta +\alpha}  \left
(\sum_{l=1}^d \|P_{N_1 ,e_l}P_{N_1} F_1\|_{L_{e_l}^{\frac{4}{\varepsilon} ,\frac{4}{2-\varepsilon}}}  \sum_{l=1}^d\|P_{N_2}v_2\|_{L_{e_l}^{\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}}} \right)^{\theta_1} \\
&\qquad \times ( \| P_{N_1} F_1 \|_{L_t^{q_3} L_x^{q_3}} \|P_{N_2} v_2\|_{L_t^{q_2} L_x^{q_2}})^{1-\theta_1}  \|P_{N_3} v_3\|_{L^{\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}}_e}\\
&\lesssim  N^{\beta +\alpha} (\langle N_1\rangle^{-A} \|P_{N_1} F_1\|_{Y_{N_1}} N_2^{ \alpha (\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}) - \alpha } \|P_{N_2}v_2\|_{X_{N_2}} )^{\theta_1} \\
&\qquad \times  (\langle N_1 \rangle^{-C}\|P_{N_1} F_1\|_{Y_{N_1}} N_2^{-\alpha} \|P_{N_2} v_2 \|_{X_{N_2}} )^{1-\theta_1}  N_3^{\alpha (\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}) -\alpha}\|P_{N_3} v_3\|_{X_{N_3}}.
\end{aligned}
\end{multline*}
Notice that $\alpha (\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}) - \alpha= \frac{s-1}{2}-\varepsilon \frac{d+s-2}{4}>0 $, since we assumed 
$\varepsilon < 2\frac{s-1}{d+s-2}$. 
Also, $2\alpha - \alpha (\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon})= \frac{d+1-2s}{2}+ \varepsilon \frac{d+s-2}{4}>0$ holds true, since it is valid for $s\leq \frac{d+1}{2}$ 
and for $s> \frac{d+1}{2}$ it follows from the assumption $\varepsilon > 2\frac{2s-d-1}{d+s-2}$. 
Let us check that $\theta_1 \in (0, 1)$. Indeed, $\theta_1 > 0$ is equivalent to 
$$
\alpha \left(\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}\right) < d - s = 2 \alpha
$$
and $\theta_1 < 1$ is equivalent to 
$$
2\alpha = d - s < 2 \alpha \left(\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}\right) \,,
$$
which we just verified. 
By \eqref{relAB1}, $A$ and $C$ satisfy
$$A\theta_1 +C (1-\theta_1)>\beta +\alpha,$$
and since $\gamma \mapsto \langle N \rangle^\gamma$ on $(0, \infty)$ is monotone, from the definition of $\theta_1$ we conclude that
\begin{multline*}
N^{\beta +\alpha}  \|P_{N_1}F_1 P_{N_2} v_2 P_{N_3} v_3\|_{L^{\frac{4}{4-\varepsilon} ,\frac{4}{2+\varepsilon}}_e} \\
\lesssim \left(\frac{N}{N_1}\right)^{\beta +\alpha} \left(\frac{N_3} {N_2}\right)^{\alpha (\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}) -\alpha}   \|P_{N_1}F_1\|_{Y_{N_1}}  \|P_{N_2}v_2\|_{X_{N_2}}   
\|P_{N_3}v_3\|_{X_{N_3}} .
\end{multline*}

\textbf{Term (7)}: $F_1 F_2 v_3$.
Proceeding as in Term (6), with $(P_{N_2}v_2, P_{N_3}v_3)$ replaced by $(P_{N_3}v_3, P_{N_2}F_2)$ and $\theta_1$ by $\theta_2$ we have
\begin{multline*}
N^{\beta +\alpha} \|P_{N_1}F_1  P_{N_2} F_2 P_{N_3} v_3\|_{L^{\frac{4}{4-\varepsilon} ,\frac{4}{2+\varepsilon}}_e}\\
\begin{aligned}
 & \lesssim  N^{\beta +\alpha} \left(\sum_{l=1}^d \|P_{N_1 ,e_l}P_{N_1} F_1\|_{L_{e_l}^{\frac{4}{\varepsilon} ,\frac{4}{2-\varepsilon} }}  \|P_{N_3} v_3\|_{L_{e_l}^{\frac{4}{2-\varepsilon}, \frac{4}{\varepsilon}}} 
 \right)^{\theta_2}  \\
& \qquad \times (\|P_{N_1}F_1\|_{L_t^{q_3}L_x^{ q_3 }} \|P_{N_3}v_3 \|_{L_t^{q_2 }L_x^{q_2} } )^{1-\theta_2}  \|P_{N_2}F_2\|_{L_{e}^{\frac{4}{2-\varepsilon} ,\frac{ 4}{\varepsilon}}} \\
&\lesssim  N^{\beta +\alpha} (\langle N_1 \rangle^{-A} \| P_{N_1}F_1\|_{Y_{N_1}} N_3^{\alpha (\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}) -\alpha} \|P_{N_3}v_3\|_{X_{N_3}} )^{\theta_2} \\ 
&\qquad \times (\langle N_1\rangle^{-C}\|P_{N_1} F_1\|_{Y_{N_1}}N_3^{-\alpha} \|P_{N_3}v_3 \|_{X_{N_3}} )^{1-\theta_2}  N_2^{B}\|P_{N_2} F_2\|_{Y_{N_2}} \,,
\end{aligned}
\end{multline*}
where we used $\theta_2 \in (0, 1)$, (see \eqref{relAB2} for the definition of $\theta_2$).
Since $\theta_2 = \alpha/\alpha (\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon})$,
$$A\theta_2 +C(1-\theta_2 )> \beta +\alpha +B,$$
and monotonicity of $\gamma \mapsto \langle N \rangle^{\gamma}$ on $(0, \infty)$ yields
\begin{align*}
&N^{\beta +\alpha} \|P_{N_1}F_1  P_{N_2} F_2 P_{N_3} v_3\|_{L^{\frac{4}{4-\varepsilon} ,\frac{4}{2+\varepsilon}}_e}\\
&\qquad \lesssim \left(\frac{N}{N_1}\right)^{\beta +\alpha} \left(\frac{N_2 }{N_1}\right)^{B }
 \|P_{N_1}F_1\|_{Y_{N_1}}  \|P_{N_2}F_2\|_{Y_{N_2}}   \|P_{N_3}v_3\|_{X_{N_3}}. 
\end{align*}








\textbf{Term (8)}:  $F_1 v_2 F_3$. 
Proceeding as in Term (7), with $(P_{N_3}v_3, P_{N_2}F_2)$ replaced by $(P_{N_2}v_2, P_{N_3} F_3)$  we have
\begin{multline*}
 N^{\beta +\alpha} \|P_{N_1}F_1  P_{N_2} v_2 P_{N_3} F_3\|_{L^{\frac{4}{4-\varepsilon} ,\frac{4}{2+\varepsilon}}_e} \\
\begin{aligned}
& \lesssim N^{\beta +\alpha} \left( \sum_{l=1}^d\|P_{N_1 ,e_l}P_{N_1} F_1\|_{L_{e_l}^{\frac{4}{\varepsilon} ,\frac{4}{2-\varepsilon} }}  \|P_{N_2} v_2\|_{L_{e_l}^{\frac{4}{2-\varepsilon}, \frac{4}{\varepsilon}}} \right)^{\theta_2} \\
&\qquad \times (\|P_{N_1}F_1\|_{L^{q_3}_t L_x^{ q_3 }} \|P_{N_2} v_2 \|_{L_t^{q_2 }L_x^{q_2} } )^{1-\theta_2}  \|P_{N_3} F_3\|_{L_e^{\frac{4}{2-\varepsilon} , \frac{4}{\varepsilon}}} \\
&\lesssim  N^{\beta +\alpha} (\langle N_1 \rangle^{-A} \|P_{N_1}F_1\|_{Y_{N_1}} N_2^{\alpha (\frac{4}{2-\varepsilon} ,\frac{4}{\varepsilon}) -\alpha} \|P_{N_2} v_2\|_{X_{N_2}} )^{\theta_2} \\
& \qquad \times (\langle N_1\rangle^{-C}\|P_{N_1}F_1\|_{Y_{N_1}}N_2^{-\alpha} \|P_{N_2}v_2 \|_{X_{N_2}} )^{1-\theta_2}  N_3^{B}\|F_3\|_{Y_{N_3}} \\
&\lesssim \left(\frac{N}{N_1}\right)^{\beta +\alpha}
\left(\frac{N_3 }{N_1}\right)^{ B}   \|P_{N_1}F_1\|_{Y_{N_1}}  \|P_{N_3 }F_3\|_{Y_{N_3}}   \|P_{N_2} v_2\|_{X_{N_2}},
\end{aligned}
\end{multline*}
where in the last step we used \eqref{relAB2}. 
\end{proof}











































































































\section{Almost sure local local well posedness of cubic NLS}\label{sec:random}
This section is devoted to the proof of Theorem \ref{mainthm}, that is, to the local almost sure well-posedness of the cubic NLS. Before proceeding, 
we first recall some probabilistic facts, that help us establish an almost sure bound on the 
$Y(\R)$ norm of the free evolution of the random data which, in turn, allows us to prove the local existence result.

\subsection{Probabilistic preliminaries}   

We recall a large deviation estimate proved in \cite[Lemma 3.1]{MR2425133}. We let $\|F\|_{L^p_\omega}$ denote $(\mathbb{E} |F|^{p})^{1/p}$.

\begin{lem}
\label{lemproba1}
Let $\{g_n\}_{n=1}^\infty$ be a sequence of real valued, independent, zero mean, random variables associated with the distributions $\{\mu_n\}_{n=1}^\infty$ on a probability space $(\Omega , \mathcal{A} ,\mathbb{P})$. Assume that there exists $c>0$ such that
$$\left|\int_{-\infty}^\infty e^{\gamma x} d\mu_n (x)\right| \leq e^{c\gamma^2},\ \forall \gamma\in \R \ and\ n\in \N. $$
 Then, there exists $\alpha>0$ such that for any $\lambda >0$ and any sequence $\{c_n\}_{n=1}^\infty \in l^2 (\N ; \mathbb{C})$,
$$\mathbb{P} \Big(\Big\{ \omega : \Big|\sum_{n=1}^\infty c_n g_n (\omega )\Big| >\lambda \Big\} \Big) \leq 2 e^{-\alpha \frac{\lambda^2}{\sum_n |c_n|^2}}.$$
Hence,  there exists $C>0$ such that for $2\leq p<\infty$ and every $\{ c_n\}_{n=1}^\infty \in l^2 (\N ; \mathbb{C} )$
$$\Big\|\sum_{n=1}^\infty c_n g_n (\omega)\Big\|_{L_\omega^p (\Omega)} \leq C \sqrt{p} \bigg(\sum_{n=1}^\infty |c_n|^2\bigg)^{1/2}.$$
\end{lem}


The  proof of the next lemma is a slight modification of the proof in  \cite[Lemma 4.5]{tzvetkov}. 

\begin{lem}
\label{lemproba2}
Let $F$ be a real valued measurable function on a probability space $(\Omega ,\mathcal{A} ,\mathbb{P})$. Suppose that there exists $C_0>0$, $K>0$, and $p_0 \geq 1$ such that for any $p\geq p_0$ we have
$$\|F\|_{L_\omega^p (\Omega)} \leq \sqrt{p} C_0 K.$$
Then, there exist $c>0$ and $C_1>0$ depending on $C_0$ and $p_0$, but independent of $K$, such that for every $\lambda>0$,
$$\mathbb{P} (\{ \omega \in \Omega : |F (\omega)|>\lambda \} ) \leq C_1 e^{- c\lambda^2 /K^2}.$$
 In particular, we have
$$\mathbb{P} (\{ \omega \in \Omega : |F(\omega)|<\infty \}) =1.$$
\end{lem}

\subsection{Almost sure bounds for the $Y(\R)$ norm}

Our aim in this subsection is to establish an almost sure bound for the $Y(\R)$ norm of the free evolution of the random data. The proof is based on 
a maximal function estimate for unit-scale frequency localized data, which is analogous to  \eqref{IK1}. Recall that the projection $Q_n$ was defined in \eqref{proj1}.

\begin{lem}
\label{lemIO}
Let \(P\) satisfy the conditions \eqref{aonth} and \eqref{hots}. Then for all $n\in \Z^d$ large enough and for any $l=1,\ldots ,d$ we have 
\begin{equation*}
\|e^{-it P } Q_n f\|_{L_{e_l}^{2,\infty} (\R \times \R^d)}\lesssim |n|^{\frac{s-1}{2}} \|Q_n f\|_{L_x^2 (\R^d )}.
\end{equation*}
\end{lem}
\begin{rmq}
Notice that the exponent of \(n\) in the above bound depends only on the degree of the operator \(P\) and not on the dimension, unlike in the case of bounds for the Littlewood-Paley projections of Lemma \ref{1lemIK}.
\end{rmq}

\begin{proof}
Let us assume that \(l=1\). Fix two smooth cut-off function $\chi\in C^{\infty}_{c}(B_{1})$ and $\eta\in C^{\infty}_{c}(B_{1}^{d-1})$ such that 
$$\psi (\xi)= \psi (\xi) \chi (\xi_1) \eta (\xi') ,\qquad \xi= (\xi_1 ,\xi') \in \R^d \,,$$
where $\psi$ is as in the definition of $Q_n$. For any $n \in \Z^d$ we use the notation
\begin{equation*}
\eta_{n} (\xi') = \eta (\xi^\prime - n'),\qquad \chi_n (\xi_1) = \chi(\xi_1 - n_{1})
\end{equation*}
with \(n=(n_{1},n')\in\Z\times\Z^{d}\) so that
$$\psi (\xi -n)= \psi (\xi -n) \chi_n (\xi_1) \eta_n (\xi^\prime) ,\qquad \xi= (\xi_1 ,\xi^\prime) \in \R^d .$$
Note that the size of the support and the bounds on derivatives of $\eta_n$ and $\varphi_n$ do not depend on $n$. Denote 
$$
K_n(t,x_1, x') := \int_{\R_{\xi_1} }  \int_{\R_{\xi'}^{d-1} }e^{i (x_1 \xi_1 +x' \xi'-t p(\xi_1 , \xi')  )} \chi_n (\xi_1) \eta_n (\xi' ) d\xi' d \xi_1.
$$
By a $TT^\ast$-argument similar to that of  Lemma \ref{1lemIK}, it suffices to  show the estimate estimate $\left\|K_n\right\|_{L_{e_{1}}^{1,\infty}}\lesssim|n|^{s-1}$. Similarly as in the proof of \eqref{IK1}, we have for any $|n| \gtrsim 1$ that
$$\sup_{x \in \R^{d} ,t\in \R} |K_n (t,x_1, x')| \lesssim \min \{1, |t|^{-\frac{d}{s}}\}. $$
Note that $N^d$ is replaced by $1$ because the estimate is at a fixed scale in Fourier space and $n$ represents just a translation. This follows by uniform boundedness and support considerations on the integrand defining \(K_{n}\). In addition, as in  the proof of \eqref{IK1}, for any $|n| \gtrsim 1$ and any $|x_1| \gtrsim|t|$ we have that
\begin{align*}
K_n(t,x_1 ,x')
&= \int_{\R \times \R^{d-1}}  e^{i\big(x_1 \xi_1 +x' \xi'-t p (\xi_1, \xi') \big)} 
\\
&\qquad \times
\partial_{\xi_{1}} \left[ \frac{1}{x_1 - t\partial_{\xi_{1}}p(\xi_1, \xi')} 
\partial_{\xi_{1}}\Big[\frac{\chi_n(\xi_1) \eta_n (\xi')}{x_1 - t\partial_{\xi_{1}}p(\xi_1, \xi')} \Big]\right] d\xi_1 d\xi'
\end{align*}
The integrand above vanishes unless \(|\xi_{1}-n_{1}|+|\xi'-n|\lesssim1\) and in particular we assume that \(|(\xi_{1},\xi')|\approx|(n_{1},n')|=|n|\gtrsim1\).
By \eqref{aonth}, we have 
 $|\partial_{\xi_{1}}p(\xi)| \lesssim | \xi |^{s-1}\approx |n|^{s-1}$. Then, for  \(|x_{1}|\gtrsim|n|^{s-1}|t|\) we have 
\begin{equation*}
\frac{1}{\big|x_1 - t\partial_{\xi_{1}}p(\xi_1, \xi')\big|}\lesssim\frac{1}{|x_{1}|} \,.
\end{equation*}
Furthermore, from \eqref{aonth} and the assumption $|n| \gtrsim 1$ it follows that
\begin{equation*}
\left| \partial_{\xi_1} \Big( \frac{1}{x_1 - t\partial_{\xi_{1}}p(\xi_1+n_1, \xi' + n)} \Big) \right| =
\left| \frac{t\partial_{\xi_{1},\xi_{1}}^{2}p(\xi_1, \xi')}{(x_1 - t\partial_{\xi_{1}}p(\xi_1, \xi'))^2} \right|
\lesssim \frac{|t||n|^{s-2}}{|x_1|^{2}}
 \lesssim \frac{1}{|x_1|}
\end{equation*}
and similarly
$$
\left| \partial^2_{\xi_1\xi_1}p \Big( \frac{1}{x_1 - t\partial_{\xi_{1}}p(\xi_1, \xi')} \Big) \right|
 \lesssim \frac{1}{|x_1|^2} \,.
$$
Since $|\partial_{\xi_1} \chi_n(\xi_1)| \lesssim 1$ and it is supported in \(B_{1}(n_{1})\) we obtain
$$|K_n(t,x_1 ,x^\prime )|\lesssim    (1+ |x_1| )^{-2}.$$
A combination of the above estimates gives us
\begin{align*}
\sup_{t\in \R , x^\prime \in \R^{d-1}} |K_n (t,x_1, x^\prime)|& \lesssim \frac{1}{(1+ |x_1|)^{2}}  +\min (1 , 1_{\{|x_1| \lesssim |n|^{s - 1} |t|\} }  |t|^{-\frac{d}{s}})\\
& \lesssim \frac{1}{(1+ |x_1|)^{2}}  + \min \bigg(1 ,  \bigg(\frac{|n|^{s - 1}}{|x_1|}\bigg)^{\frac{d}{s} } \bigg)\\
&\lesssim \frac{1}{(1+ |x_1|)^{2}}  +\frac{1}{\Big(1+|n|^{1-s}|x_1|\Big)^{\frac{d}{s}}}
.
\end{align*}
Then, the integration in $x_1$ yields the desired result.
\end{proof}


We are now in position to estimate almost surely the $Y(\R)$ norm of our randomized initial data. Recall that $Y(I )$, for $I\subset \R$ is of the form
$$\|f\|_{Y(I)} = \left(\sum_{N\in 2^{\Z}}  \|P_N f\|_{Y_N(I)}^2\right)^{\frac{1}{2}},$$
with
\begin{align*}
\|P_N f\|_{Y_N(I)}&= \langle N\rangle^C \left(\|P_N f\|_{L_t^{\frac{2(s+d)}{d}} L_x^{\frac{2(s+d)}{s}} (I\times \R^d)} +\|P_N f\|_{L_t^{\frac{2(s+d)}{s}} L_x^{\frac{2(s+d)}{s}}(I\times \R^d) } \right)\\
&+\langle N\rangle^A \sum_{l=1}^d \|P_{N,e_l} P_N f \|_{L^{\frac{4}{\varepsilon},\frac{4}{2-\varepsilon}}_{e_l}(I\times \R^d) } +N^{-B} \sum_{l=1}^d \|P_N f\|_{L^{\frac{4}{2-\varepsilon},\frac{4}{\varepsilon}}_{e_l} (I\times \R^d) },  
\end{align*}
for some positive constants $A,B$, and $C$. Although we imposed restrictions on $A$, $B$, $C$ above, they are not important in the following result which treats linear evolution. 


\begin{prop}
\label{estY}
Let $S= \max\{C, A+\beta (\frac{4}{\varepsilon} , \frac{4}{2-\varepsilon}), -B+\frac{s-1}{2} + \varepsilon  \frac{d -3s+2}{4} \}$ and $\varepsilon \in (0, 1)$. Let $f\in H_x^S (\R^d)$ and denote by $f^\omega$ the randomization of $f$. Then, there exist constants $C>0$ and $c>0$ such that for any $\lambda >0$, there holds
$$\mathbb{P} \left(\{ \omega \in \Omega : \| e^{-it P} f^\omega \|_{Y(\R)} >\lambda \} \right) \leq C e^{-\frac{c \lambda^2}{ \|f\|_{H_x^s (\R^d )}^{2}} }.$$
In particular, almost surely we have
$$\| e^{-it P} f^\omega \|_{Y(\R)} <\infty. $$
\end{prop}

\begin{proof}
First, for any function $F$ and any $p \geq 2$ we have by Minkowski's inequality 
\begin{align*}
\|\|F \|_{Y(I)} \|_{L_\omega^p} &=  \left(\mathbb{E}\left(\sum_{N \in 2^\Z} \|P_N F\|_{Y_N(I)}^2 \right)^{\frac{p}{2}}\right)^{\frac{1}{p}} = 
\left( \left\|\sum_{N \in 2^\Z} \|P_N F\|_{Y_N(I)}^2 \right\|_{L_\omega^{p/2 }} \right)^{\frac{1}{2}} \\
&\leq 
\left(\sum_{N \in 2^\Z} \left\| \|P_NF\|_{Y_N(I)} \right\|_{L_\omega^{p }}^2 \right)^{\frac{1}{2}} \,.
\end{align*}
Let us estimate the three terms in $Y_N(\R)$ independently. We begin with the terms multiplied by $\langle N \rangle^C$. To simplify the notation,  set $q_2=\frac{2(s+d)}{d}$ and $q_3=\frac{2(s+d)}{s}$
and assume that the norm of an intersection of spaces is the sum of norms.  Using Lemma \ref{lemproba1},
Minkowski inequality, \eqref{usbe} with $(r_1, r_2) = (\frac{2dq_3 }{dq_3 -2s}, q_3)$ and $(r_1, r_2) = (q_2, q_3)$, the Strichartz estimate \eqref{strichartze1}, we obtain for any $p \geq q_i > 2$, $i = 2, 3$
\begin{multline*}
\left\|\|P_{N}e^{-it P} f^\omega \|_{L_t^{q_2} L_x^{q_3} \cap L_t^{q_3} L_x^{q_3} (\R \times \R^d) }  \right\|_{L_\omega^p}\\
\begin{aligned}
&\leq \Big\|  \Big\| \sum_{k \in \Z^d} g_k (\omega) e^{-it P}  Q_k P_N f \Big\|_{L_\omega^p} \Big\|_{L_t^{q_2} L_x^{q_3} \cap L_t^{q_3} L_x^{q_3} (\R \times \R^d) } \\
&\lesssim \sqrt{p} \Big\| \Big( \sum_{k \in \Z^d}  |e^{-it P}  Q_k  P_N f|^2 \Big)^{\frac{1}{2}}  \Big\|_{L_t^{q_2} L_x^{q_3} \cap L_t^{q_3} L_x^{q_3} (\R\times \R^d) } \\
&\lesssim  \sqrt{p} \Big( \sum_{k \in \Z^d} \| e^{-it P}  Q_k P_N f  \|^2_{L_t^{q_2} L_x^{q_2} \cap L_t^{q_3} L_x^{\frac{2dq_3 }{dq_3 -2s}} (\R \times \R^d)}  \Big)^{1/2}\\
&\lesssim   \sqrt{p} \Big( \sum_{k \in \Z^d} \|  Q_k P_N f  \|^2_{L_x^2 (\R^d) } \Big)^{1/2}\\
&\lesssim \sqrt{p} \|P_N f\|_{L_x^2 (\R^d)} \,.
\end{aligned}
\end{multline*}
Next, we estimate the second component of $Y_N (\R)$.  As above, Lemma \ref{lemmaIK} yields for any $p \geq \max\{\frac{4}{\varepsilon}, \frac{4}{2 - \varepsilon}\}$
\begin{multline*}
\bigg\|\sum_{l=1}^d \|e^{-it P} P_{N,e_l} P_N f^\omega \|_{L^{\frac{4}{\varepsilon},\frac{4}{2-\varepsilon}}_{e_l} (\R\times \R^d)} \bigg\|_{L_\omega^p}  \\
\begin{aligned}
 &\lesssim \sqrt{p} \bigg( \sum_{l = 1}^d \sum_{k \in \Z^d} \|P_{N,e_l} P_N Q_k f \|^2_{L^{\frac{4}{\varepsilon},\frac{4}{2-\varepsilon}}_{e_l} (\R\times \R^d)} \bigg)^{\frac{1}{2}} \\
& \lesssim \sqrt{p}N^{\beta (\frac{4}{\varepsilon},\frac{4}{2-\varepsilon})} \|P_N f\|_{L_x^2 (\R^d )}.
\end{aligned}
\end{multline*}
Finally, we estimate the last component of $Y_N (\R)$. We consider two cases depending on the size of $N$ and we first  assume that $N$ is large. Then,
from triangle inequality, interpolation,  Lemma \ref{lemIO}, and Lemma \ref{lemmaIK}, it follows for any $p \geq \max (\frac{4}{2-\varepsilon},\frac{4}{\varepsilon})$
\begin{align*}
\bigg\|\sum_{l=1}^d\|e^{-it P}  &P_N f^\omega \|_{ L^{\frac{4}{2-\varepsilon},\frac{4}{\varepsilon}}_{e_l} } \bigg\|_{L^p_\omega}
\lesssim \sqrt{p} \bigg(\sum_{l=1}^d \sum_{k \in Z^d} \| e^{-it P}  P_N Q_k f \|^2_{L^{\frac{4}{2-\varepsilon},\frac{4}{\varepsilon}}_{e_l} (\R\times \R^d)}\bigg)^{\frac{1}{2}}\\
&\lesssim  \sqrt{p} \bigg(\sum_{l=1}^d \sum_{k \in Z^d} \|e^{-it P}  P_N Q_k f\|_{L^{2,\infty}_{e_l} (\R \times \R^d)}^{2(1-\varepsilon)} \|e^{-it P}  P_N Q_k f\|_{L^{4,4}_{e_l}(\R \times \R^d) }^{2\varepsilon} 
\bigg)^{\frac{1}{2}}\\
&\lesssim\sqrt{p} \bigg( \sum_{k \in Z^d} |k|^{(s-1)(1-\varepsilon)} N^{\varepsilon \frac{d -s}{2} } \|P_N Q_k f^\omega\|^2_{L_x^2 (\R^d)} \bigg)^{\frac{1}{2}}.
\end{align*}
Since the multiplier of $P_N$ and $Q_k$ is respectively non-zero only if $\frac{\xi}{N} \approx 1$ and $|\xi - k| \approx 1$, then for large $N$, one has $|k|\approx N$, and therefore 
for large $N$
\begin{align*}
&\left\|\sum_{l=1}^d \|e^{-it P}  P_N f^\omega \|_{  L^{\frac{4}{2-\varepsilon},\frac{4}{\varepsilon}}_{e_l} (\R\times \R^d)} \right\|_{L_\omega^p}
\lesssim \sqrt{p}\langle N\rangle^{\frac{s-1}{2} + \varepsilon  \frac{d -3s+2}{4}} \|P_N f\|_{L_2^2 (\R^d )}.
\end{align*}
Finally, we assume $N \lesssim 1$. Similarly as above, by using Lemma \ref{lemmaIK} and noticing that $\alpha (\frac{4}{2-\varepsilon},\frac{4}{\varepsilon} )= \frac{d-1}{2}- \varepsilon \frac{d+s -2}{4}>0$ if 
$\varepsilon < 1$ and $\frac{4}{2 - \varepsilon} < \frac{4}{\varepsilon}$
\begin{align*}
\Big\|\sum_{l=1}^d \|e^{-it P}  P_N f^\omega \|_{ L^{\frac{4}{2-\varepsilon},\frac{4}{\varepsilon}}_{e_l} (\R\times \R^d)} \Big\|_{L^p_\omega} 
&\lesssim  N^{\alpha (\frac{4}{2-\varepsilon},\frac{4}{\varepsilon} ) } \sqrt{p} \|P_N f\|_{L_2^2 (\R^d )}  \\
&\lesssim \sqrt{p} \|P_N f\|_{L_2^2 (\R^d )} \,,
\end{align*}
where in the last step we used $N \lesssim 1$.
A combination of previous estimates and the definition of $H^S$ space, give us
$$\| \|e^{itP}f^\omega \|_{Y(\R)} \|_{L^p_\omega} \lesssim \sqrt{p} \|f\|_{H_x^S (\R^d )}, $$
for 
$$S=\max \left\{C, A+\beta \left(\frac{4}{\varepsilon} , \frac{4}{2-\varepsilon}\right), -B+\frac{s-1}{2} + \varepsilon  \frac{d -3s+2}{4}  \right\}.$$
and we conclude by using Lemma \ref{lemproba2}.
\end{proof}

Next, we find the minimal $S$ from Proposition \ref{estY} for various choices of $s$ and $d$. 

\begin{lem}
\label{lastmainlem}
Let $A, B, C, \varepsilon \geq 0$ satisfy the assumptions of Proposition \ref{mainpropnonlinear} and Proposition \ref{estY}. Then, Proposition \ref{estY} holds for any $S > S_{\min}$, where
\begin{equation}
S_{\min} = (d - s)\times
\begin{cases}
 \frac{(d - 2s + 1)}{2(d - 1)}  & s \leq \frac{d + 2}{3} \,, \\
\frac{1}{6} & \frac{d + 2}{3} <  s \leq \frac{d + 1}{2} \,, \\
 \frac{3s - d - 2}{2(d + s - 2)}  & s  >  \frac{d + 1}{2} \,. 
\end{cases}
\end{equation}
\end{lem}

\begin{proof}
To simplify the notation, we denote $\gamma := \beta \left(\frac{4}{\varepsilon} , \frac{4}{2-\varepsilon}\right)$.
Note that the restriction $\varepsilon \leq 1$ in Proposition \ref{estY} follows from $\varepsilon < 2\frac{s - 1}{d + s - 2}$ assumed in  Proposition \ref{mainpropnonlinear}. 
Also, the condition $\varepsilon < 2\frac{s - 1}{d + s - 2}$ translates to new variables as $\gamma < 0$. 
The other condition on $\varepsilon$ in Proposition \ref{mainpropnonlinear} given by $\varepsilon > 2\frac{2s - d -1}{d+ s - 2}$ if $s > \frac{d + 1}{2}$ translates 
as $\gamma > \frac{s - d}{2}$ if $s > \frac{d + 1}{2}$.

Also, the last term in the definition of $S$ in Proposition \ref{estY} translates to new variable $\gamma$ as 
$$
-B+\frac{s-1}{2} + \varepsilon  \frac{d -3s+2}{4} = -B + (s - 1)(1 - \varepsilon) + \gamma \,.
$$
Since $d$, $s$ are fixed, we need to choose $A, B, C > 0$ and $\gamma$ satisfying \eqref{recc},  \eqref{relAB}, and \eqref{relAB1} such that 
$$S=\max \left\{C, A + \gamma, -B+ (s - 1)(1 - \varepsilon) + \gamma \right\}$$
is minimal. Let us define $B$ by \eqref{relAB}, that is, 
$$
B = \frac{1}{2}\left(A - \gamma - \frac{d-s}{2} \right)\,.
$$
Note that by increasing $C$ or $A$, we are not going to destroy validity of $A, B, C \geq 0$ or validity of \eqref{relAB1}. Thus, we can without loss of generality assume that $C = A + \gamma$, otherwise 
increase the smaller quantity, which does not increase the value of $S$. Since $C \geq \frac{s(d - s)}{4(d + s)}$, we have $A \geq \frac{s(d - s)}{4(d + s)}- \gamma > 0$.  
In addition, for $C = A + \gamma$, \eqref{relAB1} is equivalent to 
$$
A  > \theta_1 \gamma + \frac{d - s}{2} \,,
$$
which implies that
$$
B= A - \theta_1\gamma - \frac{d - s}{2} - (1 - \theta_1)\gamma \geq 0 \,,
$$
where the last inequality follows from $\gamma \leq 0$. Finally, we claim that 
$$
A + \gamma \geq -B+ (s - 1)(1 - \varepsilon) + \gamma \,.
$$
Indeed, otherwise we increase $A$, which increases $B$, keeping $A, B, C \geq 0$, and $\eqref{relAB1}$ and not increasing $S$. Substituting for $B$, we have 
$$
A  \geq  \frac{d - s}{6} + \frac{\gamma}{3} + \frac{2}{3}(s - 1)(1 - \varepsilon) \,.
$$
Overall, we showed that $S$ is minimal if $A + \gamma$ is minimal over $\gamma$ with restriction 
\begin{equation}\label{opm}
S = A + \gamma \geq \max \left\{\frac{s(d - s)}{4(d + s)},  (\theta_1 + 1) \gamma + \frac{d - s}{2},  \frac{d - s}{6} + \frac{4\gamma}{3} + \frac{2}{3}(s - 1)(1 - \varepsilon)  \right\} \,,
\end{equation}
where $\varepsilon = (\gamma + \frac{s - 1}{2}) \frac{4}{d + s - 2}$, $\theta_1 = \frac{(d - s)/2 + \gamma}{(d - s)/2 - \gamma}$ and we can take minimum of right hand sides with respect to $\gamma \in (\frac{1-s}{2}, 0)$ and 
$\gamma \in ( \frac{s - d}{2}, 0)$ if $s > \frac{d + 1}{2}$. Since $s \geq 2$ and $d \geq 3$, it is easy to check that the all terms in the maximum in \eqref{opm} are increasing in $\gamma$. Thus, the infimum 
is attained at $\gamma = \frac{1-s}{2}$ if $s \leq \frac{d + 1}{2}$ and at $\gamma =  \frac{s - d}{2}$ if $s > \frac{d + 1}{2}$. A substitution and assumption $d > s$ yield
\begin{equation}\label{sft}
S > \max \left\{\frac{s(d - s)}{4(d + s)},  \frac{(d - s)(d - 2s + 1)}{2(d - 1)}, \frac{d - s}{6}  \right\} \,,  \qquad s \leq \frac{d + 1}{2}\,.
\end{equation}
and 
\begin{equation}\label{sft2}
S > \max\left\{\frac{s(d - s)}{4(d + s)},  (d - s)\frac{3s - d - 2}{2(d + s - 2)}\right\}\,,   \qquad s  >  \frac{d + 1}{2}\,.
\end{equation}
Note that since $d > s$, we have $\frac{s(d - s)}{4(d + s)} < \frac{d - s}{6}$, and the first term in the maximum in \eqref{sft} can be removed. We claim that $\frac{s(d - s)}{4(d + s)}$
can be removed in \eqref{sft2} as well. Indeed, since $s > \frac{d+1}{2}$, and $d \geq 3$, 
\begin{align*}
\frac{3s - d - 2}{2(d + s - 2)} &> \frac{3s - d - 2}{2(d + s )} = \frac{s + 5s - 2d - 4}{4(d + s )} > \frac{s + \frac{5}{2}(d + 1) - 2d - 4}{4(d + s )} \\
&= \frac{s + \frac{1}{2}d  - \frac{3}{2}}{4(d + s )} \geq \frac{s}{4(d + s )} 
\end{align*}
The assertion follows after the maximum (of last two terms) in \eqref{sft} is identified.
\end{proof}

\subsection{Proof of Theorem \ref{mainthm}}
In this subsection, we  prove Theorem \ref{mainthm}, which is an immediate consequence of the local well-posedness for a forced cubic equation and the bound on $\|e^{-itP}f^\omega\|_{Y(\R)}$,
 established in Proposition \ref{estY}. Specifically, we consider the problem
\begin{equation}
\label{finaleq}
\begin{cases} (i\partial_t -P) v = \pm |F+v|^2 (F+v),\\ v(t_0)=v_0 \in \dot{H}_x^\alpha (\R^d) \end{cases}
\end{equation}
for some $F:\R \times \R^d \rightarrow \mathbb{C}$ such that $\|F\|_{Y(\R)} <\infty$. Recall that $\alpha = \frac{d-s}{2}$. We have the following local well-posedness result.

\begin{prop}
\label{exlocforced}
Let $t_0 \in \R$ and let $I$ be an open time interval containing $t_0$. Let $F \in Y(\R)$ and $v_0 \in \dot{H}_x^\alpha (\R^d)$. There exists $0<\delta <<1$ such that if
$$\|e^{-i(t-t_0) P} v_0\|_{X(I)}+\|F\|_{Y(I)}\leq \delta,$$
then there exists a unique solution
$$v\in C(I; \dot{H}_x^\alpha (\R^d)) \cap X(I)$$
to \eqref{finaleq} on $I\times \R^d$. 
\end{prop}

\begin{proof}
We assume without loss of generality that $t_0=0$ and let $\delta>0$. We use a fixed point argument to construct our solution. We define
$$\mathcal{B} = \{v\in X(I) : \|v\|_{X(I) } \leq 2\delta\}$$
and the map
$$\Phi (v)(t) = e^{-itP}v_0 \mp \int_0^t e^{-i(t-s)P} |F+v|^2 (F+v)(s) ds, $$
which, by Duhamel's formula, is a solution of the linear Schr\" odinger with the right hand side $ |F+v|^2 (F+v)$ and zero initial condition. 
Next,  Proposition \ref{mainproplinearest} and Corollary \ref{mainpropnonlinear}, imply for $v\in \mathcal{B}$,
\begin{align*}
\|\Phi (v)\|_{X(I)}& \leq  C \||F+v|^2 (F+v)\|_{G(I)} \leq C (\|v\|_{X(I)}^3 +\|F\|_{Y(I)}^3) \leq 2\delta,
\end{align*}
provided that $\delta$ is small enough. In addition,  fo any $v_1 ,v_2 \in \mathcal{B}$,
\begin{align*}
\|\Phi (v_1) - \Phi (v_2)\|_{X(I)}&\leq C \||F+v_1|^2 (F+v_1) - |F+v_2|^2 (F+v_2)\|_{G(I)}\\
&\leq C \|v_1 - v_2\|_{X(I)} (\|v_1\|^2_{X(I)} \|v_2\|_{X(I)}^2 +\|F\|_{Y(I)}^2)\\
&\leq \dfrac{\|v_1 - v_2\|_{X(I)}}{2} .
\end{align*}
So $\Phi : \mathcal{B} \rightarrow \mathcal{B}$ is a contraction with respect to the $X(I)$ norm. Thus there exists a unique solution to \eqref{finaleq}.
\end{proof}

We are now in position to prove Theorem \ref{mainthm}.


\begin{proof}[Proof of Theorem \ref{mainthm}]
We are looking for a solution to \eqref{eqintro} of the form
$$u(t)=e^{-itP} f^\omega +v(t) \,,$$
where $v$ is a solution to
\begin{equation}
\label{eqf2}
\begin{cases}(i\partial_t - P)v = \pm |e^{itP } f^\omega +v|^2 (e^{-itP}f^\omega +v)\qquad \textrm{on}\ I\times \R^d ,\\ v(0)=0.\end{cases}
\end{equation}
Since by  Proposition \ref{estY} one has $\|e^{itP}f^\omega\|_{Y(\R)}<\infty$, for a.e. $\omega \in \Omega$, then by Lemma \ref{lemcontt}, we can find an interval $I^\omega \subset \R$
with $0 \in I^\omega$ such that $\|e^{-itP}f^\omega\|_{Y(I^\omega )}\leq \delta$, where $0<\delta \ll1$ is the constant given in Proposition \ref{exlocforced}, and consequently 
there exists a unique solution $v\in C(I^\omega ; \dot{H}_x^\alpha (\R^d)) \cap X(I^\omega)$ to \eqref{eqf2} for a.e. $\omega \in \Omega$.
To show global uniqueness, assume that $w$ is a solution of \eqref{eqf2} belonging to $C(I^\omega ; \dot{H}_x^\alpha (\R^d)) \cap X(I^\omega)$. 
Then, from the continuity and $w(0) = 0$ follows that $w$ is small for short times, and therefore by the uniqueness of small solutions, $w = v$. Iterating this procedure, we obtain uniqueness on $I^\omega$. 
\end{proof}






\bibliographystyle{alpha}
\bibliography{biblo}


\end{document}
